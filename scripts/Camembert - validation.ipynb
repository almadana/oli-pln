{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8166bdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CamembertForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=32005, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import CamembertForMaskedLM,CamembertTokenizer\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcaed1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4b61c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "camembert_fill_mask  = pipeline(\"fill-mask\", model=\"camembert-base\", tokenizer=\"camembert-base\")\n",
    "results = camembert_fill_mask(\"Le camembert est <mask> :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce527db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "camembert_embed = pipeline(\"feature-extraction\",model=\"camembert-base\",tokenizer=\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb603ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4909106194972992,\n",
       "  'token': 7200,\n",
       "  'token_str': 'délicieux',\n",
       "  'sequence': 'Le camembert est délicieux :)'},\n",
       " {'score': 0.10556956380605698,\n",
       "  'token': 2183,\n",
       "  'token_str': 'excellent',\n",
       "  'sequence': 'Le camembert est excellent :)'},\n",
       " {'score': 0.034533172845840454,\n",
       "  'token': 26202,\n",
       "  'token_str': 'succulent',\n",
       "  'sequence': 'Le camembert est succulent :)'},\n",
       " {'score': 0.033031292259693146,\n",
       "  'token': 528,\n",
       "  'token_str': 'meilleur',\n",
       "  'sequence': 'Le camembert est meilleur :)'},\n",
       " {'score': 0.03007633611559868,\n",
       "  'token': 1654,\n",
       "  'token_str': 'parfait',\n",
       "  'sequence': 'Le camembert est parfait :)'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c52e506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fill_mask(masked_input, model, tokenizer, topk=5):\n",
    "    # Adapted from https://github.com/pytorch/fairseq/blob/master/fairseq/models/roberta/hub_interface.py\n",
    "    assert masked_input.count(\"<mask>\") == 1\n",
    "    input_ids = torch.tensor(tokenizer.encode(masked_input, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "    logits = model(input_ids)[0]  # The last hidden-state is the first element of the output tuple\n",
    "    masked_index = (input_ids.squeeze() == tokenizer.mask_token_id).nonzero().item()\n",
    "    logits = logits[0, masked_index, :]\n",
    "    prob = logits.softmax(dim=0)\n",
    "    values, indices = prob.topk(k=topk, dim=0)\n",
    "    topk_predicted_token_bpe = \" \".join(\n",
    "        [tokenizer.convert_ids_to_tokens(indices[i].item()) for i in range(len(indices))]\n",
    "    )\n",
    "    masked_token = tokenizer.mask_token\n",
    "    topk_filled_outputs = []\n",
    "    for index, predicted_token_bpe in enumerate(topk_predicted_token_bpe.split(\" \")):\n",
    "        predicted_token = predicted_token_bpe.replace(\"\\u2581\", \" \")\n",
    "        if \" {0}\".format(masked_token) in masked_input:\n",
    "            topk_filled_outputs.append(\n",
    "                (\n",
    "                    masked_input.replace(\" {0}\".format(masked_token), predicted_token),\n",
    "                    values[index].item(),\n",
    "                    predicted_token,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            topk_filled_outputs.append(\n",
    "                (masked_input.replace(masked_token, predicted_token), values[index].item(), predicted_token,)\n",
    "            )\n",
    "    return topk_filled_outputs\n",
    "\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "masked_input = \"Dans la douche, il se lave la peau avec du  <mask>\"\n",
    "#print(fill_mask(masked_input, model, tokenizer, topk=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5d247f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Le camembert est délicieux :)', 0.4909106194972992, ' délicieux'),\n",
       " ('Le camembert est excellent :)', 0.10556956380605698, ' excellent'),\n",
       " ('Le camembert est succulent :)', 0.034533172845840454, ' succulent')]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_line = 'Le camembert est <mask> :)'\n",
    "fill_mask(masked_line,model,tokenizer, topk=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "185c51e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c3a04cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15898"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"▁jambon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ded8d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = torch.tensor(tokenizer.encode(masked_line)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "428794f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = model(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0072e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0203,  0.0350,  0.0881,  ..., -0.1152,  0.0047, -0.0551],\n",
       "         [-0.0346, -0.1090,  0.0404,  ..., -0.0560, -0.0389,  0.0104],\n",
       "         [-0.0358, -0.0795,  0.1193,  ..., -0.0601, -0.1193,  0.0900],\n",
       "         ...,\n",
       "         [-0.0528,  0.0296,  0.0976,  ...,  0.0215, -0.0267, -0.1271],\n",
       "         [ 0.0712, -0.1675, -0.6567,  ...,  0.0786, -0.0880,  0.1441],\n",
       "         [-0.0380,  0.0742,  0.0933,  ..., -0.0585, -0.0465, -0.0675]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acts.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c3fbe360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be72893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "cd6128c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32004)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Le camembert est <mask> :)\"\n",
    "ids = torch.tensor(tokenizer.encode(s,add_special_tokens=True)).unsqueeze(0)\n",
    "acts_mask = model(ids)[0]\n",
    "acts_mask = acts_mask[0,-3,:]\n",
    "prob = acts_mask.softmax(dim=0)\n",
    "max_prob =max(prob)\n",
    "ids[0,-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8a65a03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3250e-08, 9.4328e-09, 5.1482e-07,  ..., 4.9397e-10, 1.0783e-08,\n",
       "        2.3439e-09], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c6da023d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acts_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ffe6b7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4909, grad_fn=<SelectBackward0>)\n",
      "tensor(1., grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "s1 = \"Le camembert est délicieux :)\"\n",
    "ids = torch.tensor(tokenizer.encode(s1,add_special_tokens=True)).unsqueeze(0)\n",
    "acts1_mask = model(ids)[-0]\n",
    "acts1_mask = acts1_mask[0,-3,:]\n",
    "word_index=ids[0,-3].item()\n",
    "\n",
    "print(prob[word_index])\n",
    "print(prob[word_index]/max_prob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9af1af48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.7666e-07, grad_fn=<SelectBackward0>)\n",
      "tensor(1.9895e-06, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "s2 = \"Le camembert est mouton :)\"\n",
    "ids = torch.tensor(tokenizer.encode(s2,add_special_tokens=True)).unsqueeze(0)\n",
    "acts2_mask = model(ids)[0]\n",
    "acts2_mask = acts2_mask[0,-3,:]\n",
    "word_index=ids[0,-3].item()\n",
    "\n",
    "print(prob[word_index])\n",
    "print(prob[word_index]/max_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d5199ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0904, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(cos(acts_mask,acts1_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ce6201e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0539, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(acts_mask,acts2_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "84ef4f39",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-18d06639d14f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macts_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m764\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0macts2_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m764\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430e6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "943d00bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour accrocher le cadre, Nicholas avait besoin d’un marteau et d’un navire clou \n",
      "\n",
      "['Pour', 'accrocher', 'le', 'cadre,', 'Nicholas', 'avait', 'besoin', 'd’un', 'marteau', 'et', 'd’un', 'navire']\n",
      "['Pour', 'accrocher', 'le', 'cadre,', 'Nicholas', 'avait', 'besoin', 'd’un', 'marteau', 'et', 'd’un', 'clou']\n",
      "Dans la douche, il se lave la peau avec du teste savon \n",
      "\n",
      "['Dans', 'la', 'douche,', 'il', 'se', 'lave', 'la', 'peau', 'avec', 'du', 'teste']\n",
      "['Dans', 'la', 'douche,', 'il', 'se', 'lave', 'la', 'peau', 'avec', 'du', 'savon']\n",
      "Elle est allée à la boulangerie pour acheter une miche de veste pain \n",
      "\n",
      "['Elle', 'est', 'allée', 'à', 'la', 'boulangerie', 'pour', 'acheter', 'une', 'miche', 'de', 'veste']\n",
      "['Elle', 'est', 'allée', 'à', 'la', 'boulangerie', 'pour', 'acheter', 'une', 'miche', 'de', 'pain']\n",
      "Katie a mis le bouquet de fleurs dans un poisson vase \n",
      "\n",
      "['Katie', 'a', 'mis', 'le', 'bouquet', 'de', 'fleurs', 'dans', 'un', 'poisson']\n",
      "['Katie', 'a', 'mis', 'le', 'bouquet', 'de', 'fleurs', 'dans', 'un', 'vase']\n",
      "Le cowboy a mis la selle sur le billet cheval \n",
      "\n",
      "['Le', 'cowboy', 'a', 'mis', 'la', 'selle', 'sur', 'le', 'billet']\n",
      "['Le', 'cowboy', 'a', 'mis', 'la', 'selle', 'sur', 'le', 'cheval']\n",
      "Le fermier ne veut pas traire sa grenade vache \n",
      "\n",
      "['Le', 'fermier', 'ne', 'veut', 'pas', 'traire', 'sa', 'grenade']\n",
      "['Le', 'fermier', 'ne', 'veut', 'pas', 'traire', 'sa', 'vache']\n",
      "La nuit, Anik verrouille la poudre porte \n",
      "\n",
      "['La', 'nuit,', 'Anik', 'verrouille', 'la', 'poudre']\n",
      "['La', 'nuit,', 'Anik', 'verrouille', 'la', 'porte']\n",
      "L’enfant se couche dans le cerveau lit\n",
      "\n",
      "['L’enfant', 'se', 'couche', 'dans', 'le', 'cerveau']\n",
      "['L’enfant', 'se', 'couche', 'dans', 'le', 'lit']\n",
      "Au parc, la fille se balance sur la chemise balançoire \n",
      "\n",
      "['Au', 'parc,', 'la', 'fille', 'se', 'balance', 'sur', 'la', 'chemise']\n",
      "['Au', 'parc,', 'la', 'fille', 'se', 'balance', 'sur', 'la', 'balançoire']\n",
      "Autour de la taille, il porte une cabine ceinture \n",
      "\n",
      "['Autour', 'de', 'la', 'taille,', 'il', 'porte', 'une', 'cabine']\n",
      "['Autour', 'de', 'la', 'taille,', 'il', 'porte', 'une', 'ceinture']\n",
      "Chloé boit son thé glacé avec une facture paille \n",
      "\n",
      "['Chloé', 'boit', 'son', 'thé', 'glacé', 'avec', 'une', 'facture']\n",
      "['Chloé', 'boit', 'son', 'thé', 'glacé', 'avec', 'une', 'paille']\n",
      "Ils sont allés voir le nouveau film au moteur cinéma \n",
      "\n",
      "['Ils', 'sont', 'allés', 'voir', 'le', 'nouveau', 'film', 'au', 'moteur']\n",
      "['Ils', 'sont', 'allés', 'voir', 'le', 'nouveau', 'film', 'au', 'cinéma']\n",
      "La petite fille s’est couchée dans son satellite lit \n",
      "\n",
      "['La', 'petite', 'fille', 's’est', 'couchée', 'dans', 'son', 'satellite']\n",
      "['La', 'petite', 'fille', 's’est', 'couchée', 'dans', 'son', 'lit']\n",
      "Les oisillons sont prêts à partir du fromage nid \n",
      "\n",
      "['Les', 'oisillons', 'sont', 'prêts', 'à', 'partir', 'du', 'fromage']\n",
      "['Les', 'oisillons', 'sont', 'prêts', 'à', 'partir', 'du', 'nid']\n",
      "L’enseignante de mathématique a écrit le problème sur le pistolet tableau \n",
      "\n",
      "['L’enseignante', 'de', 'mathématique', 'a', 'écrit', 'le', 'problème', 'sur', 'le', 'pistolet']\n",
      "['L’enseignante', 'de', 'mathématique', 'a', 'écrit', 'le', 'problème', 'sur', 'le', 'tableau']\n",
      "Ce matin le professeur a bu un robot café \n",
      "\n",
      "['Ce', 'matin', 'le', 'professeur', 'a', 'bu', 'un', 'robot']\n",
      "['Ce', 'matin', 'le', 'professeur', 'a', 'bu', 'un', 'café']\n",
      "Claude voit une annonce dans le fusil journal \n",
      "\n",
      "['Claude', 'voit', 'une', 'annonce', 'dans', 'le', 'fusil']\n",
      "['Claude', 'voit', 'une', 'annonce', 'dans', 'le', 'journal']\n",
      "Suzanne mange un cornet de trésor crème \n",
      "\n",
      "['Suzanne', 'mange', 'un', 'cornet', 'de', 'trésor']\n",
      "['Suzanne', 'mange', 'un', 'cornet', 'de', 'crème']\n",
      "Ils sont assis ensemble sans dire un seul poil mot \n",
      "\n",
      "['Ils', 'sont', 'assis', 'ensemble', 'sans', 'dire', 'un', 'seul', 'poil']\n",
      "['Ils', 'sont', 'assis', 'ensemble', 'sans', 'dire', 'un', 'seul', 'mot']\n",
      "Le chaton joue avec la pelote de fenêtre laine \n",
      "\n",
      "['Le', 'chaton', 'joue', 'avec', 'la', 'pelote', 'de', 'fenêtre']\n",
      "['Le', 'chaton', 'joue', 'avec', 'la', 'pelote', 'de', 'laine']\n",
      "Alice va à la bibliothèque pour emprunter un serpent livre \n",
      "\n",
      "['Alice', 'va', 'à', 'la', 'bibliothèque', 'pour', 'emprunter', 'un', 'serpent']\n",
      "['Alice', 'va', 'à', 'la', 'bibliothèque', 'pour', 'emprunter', 'un', 'livre']\n",
      "Jean a pêché un gros casino poisson \n",
      "\n",
      "['Jean', 'a', 'pêché', 'un', 'gros', 'casino']\n",
      "['Jean', 'a', 'pêché', 'un', 'gros', 'poisson']\n",
      "Claire a envoyé la carte d’anniversaire par la colline poste \n",
      "\n",
      "['Claire', 'a', 'envoyé', 'la', 'carte', 'd’anniversaire', 'par', 'la', 'colline']\n",
      "['Claire', 'a', 'envoyé', 'la', 'carte', 'd’anniversaire', 'par', 'la', 'poste']\n",
      "L’ambulance approche, j’entends la planète sirène \n",
      "\n",
      "['L’ambulance', 'approche,', 'j’entends', 'la', 'planète']\n",
      "['L’ambulance', 'approche,', 'j’entends', 'la', 'sirène']\n",
      "Papa a tranché la dinde avec un fauteuil couteau \n",
      "\n",
      "['Papa', 'a', 'tranché', 'la', 'dinde', 'avec', 'un', 'fauteuil']\n",
      "['Papa', 'a', 'tranché', 'la', 'dinde', 'avec', 'un', 'couteau']\n",
      "Elle coupe les légumes sur le cigare comptoir \n",
      "\n",
      "['Elle', 'coupe', 'les', 'légumes', 'sur', 'le', 'cigare']\n",
      "['Elle', 'coupe', 'les', 'légumes', 'sur', 'le', 'comptoir']\n",
      "Ils ont pris le pont pour traverser la médaille rivière \n",
      "\n",
      "['Ils', 'ont', 'pris', 'le', 'pont', 'pour', 'traverser', 'la', 'médaille']\n",
      "['Ils', 'ont', 'pris', 'le', 'pont', 'pour', 'traverser', 'la', 'rivière']\n",
      "Les abeilles récoltent du pollen pour faire du crâne miel \n",
      "\n",
      "['Les', 'abeilles', 'récoltent', 'du', 'pollen', 'pour', 'faire', 'du', 'crâne']\n",
      "['Les', 'abeilles', 'récoltent', 'du', 'pollen', 'pour', 'faire', 'du', 'miel']\n",
      "L’enseignant lui rappelle de faire son motel devoir \n",
      "\n",
      "['L’enseignant', 'lui', 'rappelle', 'de', 'faire', 'son', 'motel']\n",
      "['L’enseignant', 'lui', 'rappelle', 'de', 'faire', 'son', 'devoir']\n",
      "Ma tondeuse est brisée, je ne peux pas tondre le pantalon gazon \n",
      "\n",
      "['Ma', 'tondeuse', 'est', 'brisée,', 'je', 'ne', 'peux', 'pas', 'tondre', 'le', 'pantalon']\n",
      "['Ma', 'tondeuse', 'est', 'brisée,', 'je', 'ne', 'peux', 'pas', 'tondre', 'le', 'gazon']\n",
      "Annie a pris un mouchoir pour se moucher le bâton nez \n",
      "\n",
      "['Annie', 'a', 'pris', 'un', 'mouchoir', 'pour', 'se', 'moucher', 'le', 'bâton']\n",
      "['Annie', 'a', 'pris', 'un', 'mouchoir', 'pour', 'se', 'moucher', 'le', 'nez']\n",
      "La professeure a aiguisé son camion crayon \n",
      "\n",
      "['La', 'professeure', 'a', 'aiguisé', 'son', 'camion']\n",
      "['La', 'professeure', 'a', 'aiguisé', 'son', 'crayon']\n",
      "Il a envoyé la lettre sans un fantôme timbre \n",
      "\n",
      "['Il', 'a', 'envoyé', 'la', 'lettre', 'sans', 'un', 'fantôme']\n",
      "['Il', 'a', 'envoyé', 'la', 'lettre', 'sans', 'un', 'timbre']\n",
      "Charles se brosse les cheveux avec la batterie brosse \n",
      "\n",
      "['Charles', 'se', 'brosse', 'les', 'cheveux', 'avec', 'la', 'batterie']\n",
      "['Charles', 'se', 'brosse', 'les', 'cheveux', 'avec', 'la', 'brosse']\n",
      "Ce garçon lit beaucoup de miel livres \n",
      "\n",
      "['Ce', 'garçon', 'lit', 'beaucoup', 'de', 'miel']\n",
      "['Ce', 'garçon', 'lit', 'beaucoup', 'de', 'livres']\n",
      "Éloi allume sa pipe avec une peinture allumette \n",
      "\n",
      "['Éloi', 'allume', 'sa', 'pipe', 'avec', 'une', 'peinture']\n",
      "['Éloi', 'allume', 'sa', 'pipe', 'avec', 'une', 'allumette']\n",
      "Le chanteur a chanté une belle larme chanson \n",
      "\n",
      "['Le', 'chanteur', 'a', 'chanté', 'une', 'belle', 'larme']\n",
      "['Le', 'chanteur', 'a', 'chanté', 'une', 'belle', 'chanson']\n",
      "Le poète a écrit un canapé poème \n",
      "\n",
      "['Le', 'poète', 'a', 'écrit', 'un', 'canapé']\n",
      "['Le', 'poète', 'a', 'écrit', 'un', 'poème']\n",
      "Pour protéger sa tête du soleil, Nicole porte un tigre chapeau \n",
      "\n",
      "['Pour', 'protéger', 'sa', 'tête', 'du', 'soleil,', 'Nicole', 'porte', 'un', 'tigre']\n",
      "['Pour', 'protéger', 'sa', 'tête', 'du', 'soleil,', 'Nicole', 'porte', 'un', 'chapeau']\n",
      "Luc verse le jus dans un requin verre\n",
      "\n",
      "['Luc', 'verse', 'le', 'jus', 'dans', 'un', 'requin']\n",
      "['Luc', 'verse', 'le', 'jus', 'dans', 'un', 'verre']\n"
     ]
    }
   ],
   "source": [
    "validation_n400_file = open(\"../data/validation1.n400.txt\")\n",
    "cong = []\n",
    "incong = []\n",
    "for line in validation_n400_file:\n",
    "    print(line)\n",
    "    mots = line.split()\n",
    "    mots_incong = mots[0:(len(mots)-1)]\n",
    "    print(mots_incong)\n",
    "    incong.append(mots_incong)\n",
    "    mots_cong = mots[0:(len(mots)-2)]+ [mots[-1] ]\n",
    "    cong.append(mots_cong)\n",
    "    print(mots_cong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d0dcee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
