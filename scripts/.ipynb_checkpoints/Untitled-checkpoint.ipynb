{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6d81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import process_textGrid as tg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78262083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertForMaskedLM, CamembertTokenizer\n",
    "\n",
    "# You can replace \"camembert-base\" with any other model from the table, e.g. \"camembert/camembert-large\".\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "camembert = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n",
    "camembert.eval() \n",
    "\n",
    "def fill_mask(masked_input, model, tokenizer, topk=5):\n",
    "    # Adapted from https://github.com/pytorch/fairseq/blob/master/fairseq/models/roberta/hub_interface.py\n",
    "    assert masked_input.count(\"<mask>\") == 1\n",
    "    input_ids = torch.tensor(tokenizer.encode(masked_input, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "    logits = model(input_ids)[0]  # The last hidden-state is the first element of the output tuple\n",
    "    masked_index = (input_ids.squeeze() == tokenizer.mask_token_id).nonzero().item()\n",
    "    logits = logits[0, masked_index, :]\n",
    "    prob = logits.softmax(dim=0)\n",
    "    values, indices = prob.topk(k=topk, dim=0)\n",
    "    topk_predicted_token_bpe = \" \".join(\n",
    "        [tokenizer.convert_ids_to_tokens(indices[i].item()) for i in range(len(indices))]\n",
    "    )\n",
    "    masked_token = tokenizer.mask_token\n",
    "    topk_filled_outputs = []\n",
    "    for index, predicted_token_bpe in enumerate(topk_predicted_token_bpe.split(\" \")):\n",
    "        predicted_token = predicted_token_bpe.replace(\"\\u2581\", \" \")\n",
    "        if \" {0}\".format(masked_token) in masked_input:\n",
    "            topk_filled_outputs.append(\n",
    "                (\n",
    "                    masked_input.replace(\" {0}\".format(masked_token), predicted_token),\n",
    "                    values[index].item(),\n",
    "                    predicted_token,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            topk_filled_outputs.append(\n",
    "                (masked_input.replace(masked_token, predicted_token), values[index].item(), predicted_token,)\n",
    "            )\n",
    "    return topk_filled_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07934d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14979966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32038c4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FlaubertModelForMaskedLM' from 'transformers' (/home/perezoso/anaconda3/lib/python3.8/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-498ee82fd23d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlaubertModelForMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlaubertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodelname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'flaubert/flaubert_base_uncased'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'FlaubertModelForMaskedLM' from 'transformers' (/home/perezoso/anaconda3/lib/python3.8/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import FlaubertModelForMaskedLM, FlaubertTokenizer\n",
    "\n",
    "modelname = 'flaubert/flaubert_base_uncased' \n",
    "\n",
    "\n",
    "flaubert, log = FlaubertModelForMaskedLM.from_pretrained(modelname, output_loading_info=True)\n",
    "flaubert_tokenizer = FlaubertTokenizer.from_pretrained(modelname, do_lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9272fed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0d8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a889a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_n400_file = open(\"../data/validation1.n400.txt\")\n",
    "cong = []\n",
    "incong = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f8bf7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour accrocher le cadre, Nicholas avait besoin d’un marteau et d’un navire clou \n",
      "\n",
      "['Pour', 'accrocher', 'le', 'cadre,', 'Nicholas', 'avait', 'besoin', 'd’un', 'marteau', 'et', 'd’un', 'navire']\n",
      "['Pour', 'accrocher', 'le', 'cadre,', 'Nicholas', 'avait', 'besoin', 'd’un', 'marteau', 'et', 'd’un', 'clou']\n",
      "Dans la douche, il se lave la peau avec du teste savon \n",
      "\n",
      "['Dans', 'la', 'douche,', 'il', 'se', 'lave', 'la', 'peau', 'avec', 'du', 'teste']\n",
      "['Dans', 'la', 'douche,', 'il', 'se', 'lave', 'la', 'peau', 'avec', 'du', 'savon']\n",
      "Elle est allée à la boulangerie pour acheter une miche de veste pain \n",
      "\n",
      "['Elle', 'est', 'allée', 'à', 'la', 'boulangerie', 'pour', 'acheter', 'une', 'miche', 'de', 'veste']\n",
      "['Elle', 'est', 'allée', 'à', 'la', 'boulangerie', 'pour', 'acheter', 'une', 'miche', 'de', 'pain']\n",
      "Katie a mis le bouquet de fleurs dans un poisson vase \n",
      "\n",
      "['Katie', 'a', 'mis', 'le', 'bouquet', 'de', 'fleurs', 'dans', 'un', 'poisson']\n",
      "['Katie', 'a', 'mis', 'le', 'bouquet', 'de', 'fleurs', 'dans', 'un', 'vase']\n",
      "Le cowboy a mis la selle sur le billet cheval \n",
      "\n",
      "['Le', 'cowboy', 'a', 'mis', 'la', 'selle', 'sur', 'le', 'billet']\n",
      "['Le', 'cowboy', 'a', 'mis', 'la', 'selle', 'sur', 'le', 'cheval']\n",
      "Le fermier ne veut pas traire sa grenade vache \n",
      "\n",
      "['Le', 'fermier', 'ne', 'veut', 'pas', 'traire', 'sa', 'grenade']\n",
      "['Le', 'fermier', 'ne', 'veut', 'pas', 'traire', 'sa', 'vache']\n",
      "La nuit, Anik verrouille la poudre porte \n",
      "\n",
      "['La', 'nuit,', 'Anik', 'verrouille', 'la', 'poudre']\n",
      "['La', 'nuit,', 'Anik', 'verrouille', 'la', 'porte']\n",
      "L’enfant se couche dans le cerveau lit\n",
      "\n",
      "['L’enfant', 'se', 'couche', 'dans', 'le', 'cerveau']\n",
      "['L’enfant', 'se', 'couche', 'dans', 'le', 'lit']\n",
      "Au parc, la fille se balance sur la chemise balançoire \n",
      "\n",
      "['Au', 'parc,', 'la', 'fille', 'se', 'balance', 'sur', 'la', 'chemise']\n",
      "['Au', 'parc,', 'la', 'fille', 'se', 'balance', 'sur', 'la', 'balançoire']\n",
      "Autour de la taille, il porte une cabine ceinture \n",
      "\n",
      "['Autour', 'de', 'la', 'taille,', 'il', 'porte', 'une', 'cabine']\n",
      "['Autour', 'de', 'la', 'taille,', 'il', 'porte', 'une', 'ceinture']\n",
      "Chloé boit son thé glacé avec une facture paille \n",
      "\n",
      "['Chloé', 'boit', 'son', 'thé', 'glacé', 'avec', 'une', 'facture']\n",
      "['Chloé', 'boit', 'son', 'thé', 'glacé', 'avec', 'une', 'paille']\n",
      "Ils sont allés voir le nouveau film au moteur cinéma \n",
      "\n",
      "['Ils', 'sont', 'allés', 'voir', 'le', 'nouveau', 'film', 'au', 'moteur']\n",
      "['Ils', 'sont', 'allés', 'voir', 'le', 'nouveau', 'film', 'au', 'cinéma']\n",
      "La petite fille s’est couchée dans son satellite lit \n",
      "\n",
      "['La', 'petite', 'fille', 's’est', 'couchée', 'dans', 'son', 'satellite']\n",
      "['La', 'petite', 'fille', 's’est', 'couchée', 'dans', 'son', 'lit']\n",
      "Les oisillons sont prêts à partir du fromage nid \n",
      "\n",
      "['Les', 'oisillons', 'sont', 'prêts', 'à', 'partir', 'du', 'fromage']\n",
      "['Les', 'oisillons', 'sont', 'prêts', 'à', 'partir', 'du', 'nid']\n",
      "L’enseignante de mathématique a écrit le problème sur le pistolet tableau \n",
      "\n",
      "['L’enseignante', 'de', 'mathématique', 'a', 'écrit', 'le', 'problème', 'sur', 'le', 'pistolet']\n",
      "['L’enseignante', 'de', 'mathématique', 'a', 'écrit', 'le', 'problème', 'sur', 'le', 'tableau']\n",
      "Ce matin le professeur a bu un robot café \n",
      "\n",
      "['Ce', 'matin', 'le', 'professeur', 'a', 'bu', 'un', 'robot']\n",
      "['Ce', 'matin', 'le', 'professeur', 'a', 'bu', 'un', 'café']\n",
      "Claude voit une annonce dans le fusil journal \n",
      "\n",
      "['Claude', 'voit', 'une', 'annonce', 'dans', 'le', 'fusil']\n",
      "['Claude', 'voit', 'une', 'annonce', 'dans', 'le', 'journal']\n",
      "Suzanne mange un cornet de trésor crème \n",
      "\n",
      "['Suzanne', 'mange', 'un', 'cornet', 'de', 'trésor']\n",
      "['Suzanne', 'mange', 'un', 'cornet', 'de', 'crème']\n",
      "Ils sont assis ensemble sans dire un seul poil mot \n",
      "\n",
      "['Ils', 'sont', 'assis', 'ensemble', 'sans', 'dire', 'un', 'seul', 'poil']\n",
      "['Ils', 'sont', 'assis', 'ensemble', 'sans', 'dire', 'un', 'seul', 'mot']\n",
      "Le chaton joue avec la pelote de fenêtre laine \n",
      "\n",
      "['Le', 'chaton', 'joue', 'avec', 'la', 'pelote', 'de', 'fenêtre']\n",
      "['Le', 'chaton', 'joue', 'avec', 'la', 'pelote', 'de', 'laine']\n",
      "Alice va à la bibliothèque pour emprunter un serpent livre \n",
      "\n",
      "['Alice', 'va', 'à', 'la', 'bibliothèque', 'pour', 'emprunter', 'un', 'serpent']\n",
      "['Alice', 'va', 'à', 'la', 'bibliothèque', 'pour', 'emprunter', 'un', 'livre']\n",
      "Jean a pêché un gros casino poisson \n",
      "\n",
      "['Jean', 'a', 'pêché', 'un', 'gros', 'casino']\n",
      "['Jean', 'a', 'pêché', 'un', 'gros', 'poisson']\n",
      "Claire a envoyé la carte d’anniversaire par la colline poste \n",
      "\n",
      "['Claire', 'a', 'envoyé', 'la', 'carte', 'd’anniversaire', 'par', 'la', 'colline']\n",
      "['Claire', 'a', 'envoyé', 'la', 'carte', 'd’anniversaire', 'par', 'la', 'poste']\n",
      "L’ambulance approche, j’entends la planète sirène \n",
      "\n",
      "['L’ambulance', 'approche,', 'j’entends', 'la', 'planète']\n",
      "['L’ambulance', 'approche,', 'j’entends', 'la', 'sirène']\n",
      "Papa a tranché la dinde avec un fauteuil couteau \n",
      "\n",
      "['Papa', 'a', 'tranché', 'la', 'dinde', 'avec', 'un', 'fauteuil']\n",
      "['Papa', 'a', 'tranché', 'la', 'dinde', 'avec', 'un', 'couteau']\n",
      "Elle coupe les légumes sur le cigare comptoir \n",
      "\n",
      "['Elle', 'coupe', 'les', 'légumes', 'sur', 'le', 'cigare']\n",
      "['Elle', 'coupe', 'les', 'légumes', 'sur', 'le', 'comptoir']\n",
      "Ils ont pris le pont pour traverser la médaille rivière \n",
      "\n",
      "['Ils', 'ont', 'pris', 'le', 'pont', 'pour', 'traverser', 'la', 'médaille']\n",
      "['Ils', 'ont', 'pris', 'le', 'pont', 'pour', 'traverser', 'la', 'rivière']\n",
      "Les abeilles récoltent du pollen pour faire du crâne miel \n",
      "\n",
      "['Les', 'abeilles', 'récoltent', 'du', 'pollen', 'pour', 'faire', 'du', 'crâne']\n",
      "['Les', 'abeilles', 'récoltent', 'du', 'pollen', 'pour', 'faire', 'du', 'miel']\n",
      "L’enseignant lui rappelle de faire son motel devoir \n",
      "\n",
      "['L’enseignant', 'lui', 'rappelle', 'de', 'faire', 'son', 'motel']\n",
      "['L’enseignant', 'lui', 'rappelle', 'de', 'faire', 'son', 'devoir']\n",
      "Ma tondeuse est brisée, je ne peux pas tondre le pantalon gazon \n",
      "\n",
      "['Ma', 'tondeuse', 'est', 'brisée,', 'je', 'ne', 'peux', 'pas', 'tondre', 'le', 'pantalon']\n",
      "['Ma', 'tondeuse', 'est', 'brisée,', 'je', 'ne', 'peux', 'pas', 'tondre', 'le', 'gazon']\n",
      "Annie a pris un mouchoir pour se moucher le bâton nez \n",
      "\n",
      "['Annie', 'a', 'pris', 'un', 'mouchoir', 'pour', 'se', 'moucher', 'le', 'bâton']\n",
      "['Annie', 'a', 'pris', 'un', 'mouchoir', 'pour', 'se', 'moucher', 'le', 'nez']\n",
      "La professeure a aiguisé son camion crayon \n",
      "\n",
      "['La', 'professeure', 'a', 'aiguisé', 'son', 'camion']\n",
      "['La', 'professeure', 'a', 'aiguisé', 'son', 'crayon']\n",
      "Il a envoyé la lettre sans un fantôme timbre \n",
      "\n",
      "['Il', 'a', 'envoyé', 'la', 'lettre', 'sans', 'un', 'fantôme']\n",
      "['Il', 'a', 'envoyé', 'la', 'lettre', 'sans', 'un', 'timbre']\n",
      "Charles se brosse les cheveux avec la batterie brosse \n",
      "\n",
      "['Charles', 'se', 'brosse', 'les', 'cheveux', 'avec', 'la', 'batterie']\n",
      "['Charles', 'se', 'brosse', 'les', 'cheveux', 'avec', 'la', 'brosse']\n",
      "Ce garçon lit beaucoup de miel livres \n",
      "\n",
      "['Ce', 'garçon', 'lit', 'beaucoup', 'de', 'miel']\n",
      "['Ce', 'garçon', 'lit', 'beaucoup', 'de', 'livres']\n",
      "Éloi allume sa pipe avec une peinture allumette \n",
      "\n",
      "['Éloi', 'allume', 'sa', 'pipe', 'avec', 'une', 'peinture']\n",
      "['Éloi', 'allume', 'sa', 'pipe', 'avec', 'une', 'allumette']\n",
      "Le chanteur a chanté une belle larme chanson \n",
      "\n",
      "['Le', 'chanteur', 'a', 'chanté', 'une', 'belle', 'larme']\n",
      "['Le', 'chanteur', 'a', 'chanté', 'une', 'belle', 'chanson']\n",
      "Le poète a écrit un canapé poème \n",
      "\n",
      "['Le', 'poète', 'a', 'écrit', 'un', 'canapé']\n",
      "['Le', 'poète', 'a', 'écrit', 'un', 'poème']\n",
      "Pour protéger sa tête du soleil, Nicole porte un tigre chapeau \n",
      "\n",
      "['Pour', 'protéger', 'sa', 'tête', 'du', 'soleil,', 'Nicole', 'porte', 'un', 'tigre']\n",
      "['Pour', 'protéger', 'sa', 'tête', 'du', 'soleil,', 'Nicole', 'porte', 'un', 'chapeau']\n",
      "Luc verse le jus dans un requin verre\n",
      "\n",
      "['Luc', 'verse', 'le', 'jus', 'dans', 'un', 'requin']\n",
      "['Luc', 'verse', 'le', 'jus', 'dans', 'un', 'verre']\n"
     ]
    }
   ],
   "source": [
    "for line in validation_n400_file:\n",
    "    print(line)\n",
    "    mots = line.split()\n",
    "    mots_incong = mots[0:(len(mots)-1)]\n",
    "    print(mots_incong)\n",
    "    incong.append(mots_incong)\n",
    "    mots_cong = mots[0:(len(mots)-2)]+ [mots[-1] ]\n",
    "    cong.append(mots_cong)\n",
    "    print(mots_cong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "216944cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pour', 'accrocher', 'le', 'cadre,', 'Nicholas', 'avait', 'besoin', 'd’un', 'marteau', 'et', 'd’un', 'clou']\n",
      "['Dans', 'la', 'douche,', 'il', 'se', 'lave', 'la', 'peau', 'avec', 'du', 'savon']\n",
      "['Elle', 'est', 'allée', 'à', 'la', 'boulangerie', 'pour', 'acheter', 'une', 'miche', 'de', 'pain']\n",
      "['Katie', 'a', 'mis', 'le', 'bouquet', 'de', 'fleurs', 'dans', 'un', 'vase']\n",
      "['Le', 'cowboy', 'a', 'mis', 'la', 'selle', 'sur', 'le', 'cheval']\n",
      "['Le', 'fermier', 'ne', 'veut', 'pas', 'traire', 'sa', 'vache']\n",
      "['La', 'nuit,', 'Anik', 'verrouille', 'la', 'porte']\n",
      "['L’enfant', 'se', 'couche', 'dans', 'le', 'lit']\n",
      "['Au', 'parc,', 'la', 'fille', 'se', 'balance', 'sur', 'la', 'balançoire']\n",
      "['Autour', 'de', 'la', 'taille,', 'il', 'porte', 'une', 'ceinture']\n",
      "['Pour', 'accrocher', 'le', 'cadre,', 'Nicholas', 'avait', 'besoin', 'd’un', 'marteau', 'et', 'd’un', 'navire']\n",
      "['Dans', 'la', 'douche,', 'il', 'se', 'lave', 'la', 'peau', 'avec', 'du', 'teste']\n",
      "['Elle', 'est', 'allée', 'à', 'la', 'boulangerie', 'pour', 'acheter', 'une', 'miche', 'de', 'veste']\n",
      "['Katie', 'a', 'mis', 'le', 'bouquet', 'de', 'fleurs', 'dans', 'un', 'poisson']\n",
      "['Le', 'cowboy', 'a', 'mis', 'la', 'selle', 'sur', 'le', 'billet']\n",
      "['Le', 'fermier', 'ne', 'veut', 'pas', 'traire', 'sa', 'grenade']\n",
      "['La', 'nuit,', 'Anik', 'verrouille', 'la', 'poudre']\n",
      "['L’enfant', 'se', 'couche', 'dans', 'le', 'cerveau']\n",
      "['Au', 'parc,', 'la', 'fille', 'se', 'balance', 'sur', 'la', 'chemise']\n",
      "['Autour', 'de', 'la', 'taille,', 'il', 'porte', 'une', 'cabine']\n"
     ]
    }
   ],
   "source": [
    "#compare the predicted vector for final token at the end of the sentence, with actual encoded vector\n",
    "def processSentences(sentences):\n",
    "    sentence_cosines=[]\n",
    "    for sentence in sentences:\n",
    "        print(sentence)\n",
    "        word_vectors=[]\n",
    "        word_cosines = []\n",
    "        for index,word in enumerate(sentence):\n",
    "            nTokens_per_word = len(torch.tensor(flaubert_tokenizer.encode(word))) - 2 #if only one token for this word, this should be 1\n",
    "            \n",
    "            #actual sentence up to now\n",
    "            sentence_up_to_now = \" \".join(sentence[0:(index+1)])\n",
    "            sentence_up_to_now += \" ,\"\n",
    "            #sentence with lacking current word (replaced with mask)\n",
    "            sentence_up_to_mask = \" \".join(sentence[0:(index)])\n",
    "            sentence_up_to_mask += \" <mask> ,\"\n",
    "            #print(sentence_up_to_mask)\n",
    "            \n",
    "            \n",
    "            tokens = torch.tensor([flaubert_tokenizer.encode(sentence_up_to_now)]) # get tokens from FlauBERT\n",
    "            tokens_predict = torch.tensor([flaubert_tokenizer.encode(sentence_up_to_mask)]) # get tokens from FlauBERT\n",
    "            \n",
    "            all_vectors = flaubert(tokens)[0] # get embedding vectors from FlauBERT\n",
    "            all_vectors_predicted = flaubert(tokens_predict)[0]\n",
    "            \n",
    "            #if the word corresponds to many tokens, need to average between these\n",
    "            #these_vectors = all_vectors[:,[i for i in range(-1-nTokens_per_word,-1)],:]\n",
    "            these_vectors = all_vectors[:,[i for i in range(1,index+2)]]\n",
    "            sentence_vector = torch.mean(these_vectors,dim=1)\n",
    "            #predicted_vector = all_vectors_predicted[:,-2,:] #if comma is removed, this should be -2\n",
    "            predicted_vectors = all_vectors_predicted[:,[i for i in range(1,index+2)]]\n",
    "            sentence_vector_predicted = torch.mean(predicted_vectors,dim=1)\n",
    "            word_cosines.append( cos(sentence_vector,sentence_vector_predicted).item() )\n",
    "            #for each word, embed the whole sentence up to that word, and extract the last vector (ignore sentencestart and finish tokens)        \n",
    "        sentence_cosines.append(word_cosines)\n",
    "    return(sentence_cosines)\n",
    "cong_cosines = processSentences(cong[0:10])\n",
    "incong_cosines = processSentences(incong[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7a1c226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pour', 'accrocher', 'le', 'cadre,', 'Nicholas', 'avait', 'besoin', 'd’un', 'marteau', 'et', 'd’un', 'clou']\n",
      "['Dans', 'la', 'douche,', 'il', 'se', 'lave', 'la', 'peau', 'avec', 'du', 'savon']\n",
      "['Elle', 'est', 'allée', 'à', 'la', 'boulangerie', 'pour', 'acheter', 'une', 'miche', 'de', 'pain']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-bfd7d6c18777>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0msentence_cosines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_cosines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_cosines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mcong_cosines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mincong_cosines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-bfd7d6c18777>\u001b[0m in \u001b[0;36mprocessSentences\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0msentence_up_to_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflaubert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_up_to_now\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get tokens from FlauBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mall_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflaubert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get embedding vectors from FlauBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;31m#if the word corresponds to many tokens, need to average between these\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mthese_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnTokens_per_word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/flaubert/modeling_flaubert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, langs, token_type_ids, position_ids, lengths, cache, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;31m# FFN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                 \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/xlm/modeling_xlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/xlm/modeling_xlm.py\u001b[0m in \u001b[0;36mff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def processSentences(sentences):\n",
    "    sentence_cosines=[]\n",
    "    for sentence in sentences:\n",
    "        print(sentence)\n",
    "        word_vectors=[]\n",
    "        word_cosines = []\n",
    "        for index,word in enumerate(sentence):\n",
    "            nTokens_per_word = len(torch.tensor(flaubert_tokenizer.encode(word))) - 2 #if only one token for this word, this should be 1\n",
    "            sentence_up_to_now = \" \".join(sentence[0:(index+1)])\n",
    "            tokens = torch.tensor([flaubert_tokenizer.encode(sentence_up_to_now)]) # get tokens from FlauBERT\n",
    "            all_vectors = flaubert(tokens)[0] # get embedding vectors from FlauBERT\n",
    "            #if the word corresponds to many tokens, need to average between these\n",
    "            these_vectors = all_vectors[:,[i for i in range(-1-nTokens_per_word,-1)],:]\n",
    "            #print(torch.mean(these_vectors,dim=1).shape)\n",
    "            word_vectors.append(torch.mean(these_vectors,dim=1))\n",
    "            if index>0:\n",
    "                word_cosines.append( cos(word_vectors[-1],word_vectors[-2]).item() )\n",
    "            #for each word, embed the whole sentence up to that word, and extract the last vector (ignore sentencestart and finish tokens)        \n",
    "        sentence_cosines.append(word_cosines)\n",
    "    return(sentence_cosines)\n",
    "cong_cosines = processSentences(cong)\n",
    "incong_cosines = processSentences(incong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dc72ee47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method decode in module transformers.tokenization_utils_base:\n",
      "\n",
      "decode(token_ids: Union[int, List[int], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True, **kwargs) -> str method of transformers.models.flaubert.tokenization_flaubert.FlaubertTokenizer instance\n",
      "    Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
      "    tokens and clean up tokenization spaces.\n",
      "    \n",
      "    Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.\n",
      "    \n",
      "    Args:\n",
      "        token_ids (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      "            List of tokenized input ids. Can be obtained using the `__call__` method.\n",
      "        skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to remove special tokens in the decoding.\n",
      "        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not to clean up the tokenization spaces.\n",
      "        kwargs (additional keyword arguments, *optional*):\n",
      "            Will be passed to the underlying model specific decode method.\n",
      "    \n",
      "    Returns:\n",
      "        `str`: The decoded sentence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flaubert_tokenizer.decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "558c599c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5635402798652649,\n",
       "  0.3508917987346649,\n",
       "  0.17417731881141663,\n",
       "  0.893053412437439,\n",
       "  0.7915871739387512,\n",
       "  0.38590556383132935,\n",
       "  0.35764774680137634,\n",
       "  0.5424639582633972,\n",
       "  0.3225528597831726,\n",
       "  0.5380758047103882,\n",
       "  0.4277985095977783]]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cong_cosines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4969c96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5635402798652649,\n",
       "  0.3508917987346649,\n",
       "  0.17417731881141663,\n",
       "  0.893053412437439,\n",
       "  0.7915871739387512,\n",
       "  0.38590556383132935,\n",
       "  0.35764774680137634,\n",
       "  0.5424639582633972,\n",
       "  0.3225528597831726,\n",
       "  0.5380758047103882,\n",
       "  0.5217224359512329]]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incong_cosines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f19594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pour', 'accrocher', 'le', 'cadre,', 'Nicholas', 'avait', 'besoin', 'd’un', 'marteau', 'et', 'd’un', 'clou']\n",
      "['Dans', 'la', 'douche,', 'il', 'se', 'lave', 'la', 'peau', 'avec', 'du', 'savon']\n",
      "['Elle', 'est', 'allée', 'à', 'la', 'boulangerie', 'pour', 'acheter', 'une', 'miche', 'de', 'pain']\n",
      "['Katie', 'a', 'mis', 'le', 'bouquet', 'de', 'fleurs', 'dans', 'un', 'vase']\n",
      "['Le', 'cowboy', 'a', 'mis', 'la', 'selle', 'sur', 'le', 'cheval']\n",
      "['Le', 'fermier', 'ne', 'veut', 'pas', 'traire', 'sa', 'vache']\n",
      "['La', 'nuit,', 'Anik', 'verrouille', 'la', 'porte']\n",
      "['L’enfant', 'se', 'couche', 'dans', 'le', 'lit']\n",
      "['Au', 'parc,', 'la', 'fille', 'se', 'balance', 'sur', 'la', 'balançoire']\n",
      "['Autour', 'de', 'la', 'taille,', 'il', 'porte', 'une', 'ceinture']\n",
      "['Chloé', 'boit', 'son', 'thé', 'glacé', 'avec', 'une', 'paille']\n",
      "['Ils', 'sont', 'allés', 'voir', 'le', 'nouveau', 'film', 'au', 'cinéma']\n",
      "['La', 'petite', 'fille', 's’est', 'couchée', 'dans', 'son', 'lit']\n",
      "['Les', 'oisillons', 'sont', 'prêts', 'à', 'partir', 'du', 'nid']\n",
      "['L’enseignante', 'de', 'mathématique', 'a', 'écrit', 'le', 'problème', 'sur', 'le', 'tableau']\n",
      "['Ce', 'matin', 'le', 'professeur', 'a', 'bu', 'un', 'café']\n",
      "['Claude', 'voit', 'une', 'annonce', 'dans', 'le', 'journal']\n",
      "['Suzanne', 'mange', 'un', 'cornet', 'de', 'crème']\n",
      "['Ils', 'sont', 'assis', 'ensemble', 'sans', 'dire', 'un', 'seul', 'mot']\n",
      "['Le', 'chaton', 'joue', 'avec', 'la', 'pelote', 'de', 'laine']\n",
      "['Alice', 'va', 'à', 'la', 'bibliothèque', 'pour', 'emprunter', 'un', 'livre']\n",
      "['Jean', 'a', 'pêché', 'un', 'gros', 'poisson']\n",
      "['Claire', 'a', 'envoyé', 'la', 'carte', 'd’anniversaire', 'par', 'la', 'poste']\n",
      "['L’ambulance', 'approche,', 'j’entends', 'la', 'sirène']\n",
      "['Papa', 'a', 'tranché', 'la', 'dinde', 'avec', 'un', 'couteau']\n",
      "['Elle', 'coupe', 'les', 'légumes', 'sur', 'le', 'comptoir']\n",
      "['Ils', 'ont', 'pris', 'le', 'pont', 'pour', 'traverser', 'la', 'rivière']\n",
      "['Les', 'abeilles', 'récoltent', 'du', 'pollen', 'pour', 'faire', 'du', 'miel']\n",
      "['L’enseignant', 'lui', 'rappelle', 'de', 'faire', 'son', 'devoir']\n",
      "['Ma', 'tondeuse', 'est', 'brisée,', 'je', 'ne', 'peux', 'pas', 'tondre', 'le', 'gazon']\n",
      "['Annie', 'a', 'pris', 'un', 'mouchoir', 'pour', 'se', 'moucher', 'le', 'nez']\n",
      "['La', 'professeure', 'a', 'aiguisé', 'son', 'crayon']\n",
      "['Il', 'a', 'envoyé', 'la', 'lettre', 'sans', 'un', 'timbre']\n",
      "['Charles', 'se', 'brosse', 'les', 'cheveux', 'avec', 'la', 'brosse']\n",
      "['Ce', 'garçon', 'lit', 'beaucoup', 'de', 'livres']\n",
      "['Éloi', 'allume', 'sa', 'pipe', 'avec', 'une', 'allumette']\n",
      "['Le', 'chanteur', 'a', 'chanté', 'une', 'belle', 'chanson']\n",
      "['Le', 'poète', 'a', 'écrit', 'un', 'poème']\n",
      "['Pour', 'protéger', 'sa', 'tête', 'du', 'soleil,', 'Nicole', 'porte', 'un', 'chapeau']\n",
      "['Luc', 'verse', 'le', 'jus', 'dans', 'un', 'verre']\n",
      "['Pour', 'accrocher', 'le', 'cadre,', 'Nicholas', 'avait', 'besoin', 'd’un', 'marteau', 'et', 'd’un', 'navire']\n",
      "['Dans', 'la', 'douche,', 'il', 'se', 'lave', 'la', 'peau', 'avec', 'du', 'teste']\n",
      "['Elle', 'est', 'allée', 'à', 'la', 'boulangerie', 'pour', 'acheter', 'une', 'miche', 'de', 'veste']\n",
      "['Katie', 'a', 'mis', 'le', 'bouquet', 'de', 'fleurs', 'dans', 'un', 'poisson']\n",
      "['Le', 'cowboy', 'a', 'mis', 'la', 'selle', 'sur', 'le', 'billet']\n",
      "['Le', 'fermier', 'ne', 'veut', 'pas', 'traire', 'sa', 'grenade']\n",
      "['La', 'nuit,', 'Anik', 'verrouille', 'la', 'poudre']\n",
      "['L’enfant', 'se', 'couche', 'dans', 'le', 'cerveau']\n",
      "['Au', 'parc,', 'la', 'fille', 'se', 'balance', 'sur', 'la', 'chemise']\n",
      "['Autour', 'de', 'la', 'taille,', 'il', 'porte', 'une', 'cabine']\n",
      "['Chloé', 'boit', 'son', 'thé', 'glacé', 'avec', 'une', 'facture']\n",
      "['Ils', 'sont', 'allés', 'voir', 'le', 'nouveau', 'film', 'au', 'moteur']\n",
      "['La', 'petite', 'fille', 's’est', 'couchée', 'dans', 'son', 'satellite']\n",
      "['Les', 'oisillons', 'sont', 'prêts', 'à', 'partir', 'du', 'fromage']\n",
      "['L’enseignante', 'de', 'mathématique', 'a', 'écrit', 'le', 'problème', 'sur', 'le', 'pistolet']\n",
      "['Ce', 'matin', 'le', 'professeur', 'a', 'bu', 'un', 'robot']\n",
      "['Claude', 'voit', 'une', 'annonce', 'dans', 'le', 'fusil']\n",
      "['Suzanne', 'mange', 'un', 'cornet', 'de', 'trésor']\n",
      "['Ils', 'sont', 'assis', 'ensemble', 'sans', 'dire', 'un', 'seul', 'poil']\n",
      "['Le', 'chaton', 'joue', 'avec', 'la', 'pelote', 'de', 'fenêtre']\n",
      "['Alice', 'va', 'à', 'la', 'bibliothèque', 'pour', 'emprunter', 'un', 'serpent']\n",
      "['Jean', 'a', 'pêché', 'un', 'gros', 'casino']\n",
      "['Claire', 'a', 'envoyé', 'la', 'carte', 'd’anniversaire', 'par', 'la', 'colline']\n",
      "['L’ambulance', 'approche,', 'j’entends', 'la', 'planète']\n",
      "['Papa', 'a', 'tranché', 'la', 'dinde', 'avec', 'un', 'fauteuil']\n",
      "['Elle', 'coupe', 'les', 'légumes', 'sur', 'le', 'cigare']\n",
      "['Ils', 'ont', 'pris', 'le', 'pont', 'pour', 'traverser', 'la', 'médaille']\n",
      "['Les', 'abeilles', 'récoltent', 'du', 'pollen', 'pour', 'faire', 'du', 'crâne']\n",
      "['L’enseignant', 'lui', 'rappelle', 'de', 'faire', 'son', 'motel']\n",
      "['Ma', 'tondeuse', 'est', 'brisée,', 'je', 'ne', 'peux', 'pas', 'tondre', 'le', 'pantalon']\n",
      "['Annie', 'a', 'pris', 'un', 'mouchoir', 'pour', 'se', 'moucher', 'le', 'bâton']\n",
      "['La', 'professeure', 'a', 'aiguisé', 'son', 'camion']\n",
      "['Il', 'a', 'envoyé', 'la', 'lettre', 'sans', 'un', 'fantôme']\n",
      "['Charles', 'se', 'brosse', 'les', 'cheveux', 'avec', 'la', 'batterie']\n",
      "['Ce', 'garçon', 'lit', 'beaucoup', 'de', 'miel']\n",
      "['Éloi', 'allume', 'sa', 'pipe', 'avec', 'une', 'peinture']\n",
      "['Le', 'chanteur', 'a', 'chanté', 'une', 'belle', 'larme']\n",
      "['Le', 'poète', 'a', 'écrit', 'un', 'canapé']\n",
      "['Pour', 'protéger', 'sa', 'tête', 'du', 'soleil,', 'Nicole', 'porte', 'un', 'tigre']\n",
      "['Luc', 'verse', 'le', 'jus', 'dans', 'un', 'requin']\n"
     ]
    }
   ],
   "source": [
    "#compare each isolated word with the average of the previous three embedded words, averaged\n",
    "def processSentences(sentences):\n",
    "    sentence_cosines=[]\n",
    "    for sentence in sentences:\n",
    "        print(sentence)\n",
    "        word_cosines = []\n",
    "        word_vectors = []\n",
    "        for index,word in enumerate(sentence):\n",
    "            tokens = torch.tensor([flaubert_tokenizer.encode(word)])\n",
    "            nTokens_per_word = tokens.shape[1] - 2 #if only one token for this word, this should be 1\n",
    "            all_vectors = flaubert(tokens)[0] # get embedding vectors from FlauBERT\n",
    "            #if the word corresponds to many tokens, need to average between these\n",
    "            these_vectors = all_vectors[:,[i for i in range(-1-nTokens_per_word,-1)],:]\n",
    "            \n",
    "            word_vectors.append(torch.mean(these_vectors,dim=1))\n",
    "            if index>0:\n",
    "                #get the previous vectors (up to three)\n",
    "                sliding_vectors = torch.stack(word_vectors[-min(index,5)-1:-1],dim=1)\n",
    "                #get the average\n",
    "                sliding_average_vector = torch.mean(sliding_vectors,dim=1)\n",
    "                #append the cosine between the average sliding vector and the new vector\n",
    "                word_cosines.append( cos(word_vectors[-1],sliding_average_vector).item() )\n",
    "            #for each word, embed the whole sentence up to that word, and extract the last vector (ignore sentencestart and finish tokens)        \n",
    "        sentence_cosines.append(word_cosines)\n",
    "    return(sentence_cosines)\n",
    "cong_cosines = processSentences(cong)\n",
    "incong_cosines = processSentences(incong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b946e56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.26589128375053406,\n",
       "  0.29442495107650757,\n",
       "  0.3313521146774292,\n",
       "  0.4501274526119232,\n",
       "  0.482605516910553,\n",
       "  0.6113255023956299,\n",
       "  0.4121529459953308,\n",
       "  0.4313771724700928,\n",
       "  0.5202133059501648,\n",
       "  0.7124029397964478,\n",
       "  0.5083733201026917]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cong_cosines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6ca5cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5083733201026917,\n",
       " 0.5936498641967773,\n",
       " 0.6398859620094299,\n",
       " 0.6011548638343811,\n",
       " 0.6553562879562378,\n",
       " 0.5325997471809387,\n",
       " 0.5184604525566101,\n",
       " 0.6068021059036255,\n",
       " 0.6320134997367859,\n",
       " 0.6611517667770386,\n",
       " 0.24919062852859497,\n",
       " 0.45377692580223083,\n",
       " 0.7005411982536316,\n",
       " 0.4461159110069275,\n",
       " 0.4662097096443176,\n",
       " 0.5487198829650879,\n",
       " 0.5942673087120056,\n",
       " 0.439497172832489,\n",
       " 0.6179675459861755,\n",
       " 0.427564412355423,\n",
       " 0.5603389739990234,\n",
       " 0.6866742372512817,\n",
       " 0.3807833790779114,\n",
       " 0.5537118315696716,\n",
       " 0.5915652513504028,\n",
       " 0.6296530961990356,\n",
       " 0.5419549942016602,\n",
       " 0.5212622880935669,\n",
       " 0.6152185797691345,\n",
       " 0.6432318687438965,\n",
       " 0.5586560964584351,\n",
       " 0.5323050618171692,\n",
       " 0.6171470284461975,\n",
       " 0.7791458964347839,\n",
       " 0.42512738704681396,\n",
       " 0.4492625892162323,\n",
       " 0.6823616623878479,\n",
       " 0.4706116020679474,\n",
       " 0.7140650153160095,\n",
       " 0.5880354642868042]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[-1] for x in cong_cosines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc548296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.47868260741233826,\n",
       " 0.6253886818885803,\n",
       " 0.6998618245124817,\n",
       " 0.6338027715682983,\n",
       " 0.5714553594589233,\n",
       " 0.5927881002426147,\n",
       " 0.511491596698761,\n",
       " 0.2436196506023407,\n",
       " 0.5941759943962097,\n",
       " 0.5281726717948914,\n",
       " 0.6192193627357483,\n",
       " 0.5699337124824524,\n",
       " 0.5186161398887634,\n",
       " 0.41064879298210144,\n",
       " 0.4423866868019104,\n",
       " 0.49880173802375793,\n",
       " 0.5213174819946289,\n",
       " 0.6343529224395752,\n",
       " 0.5438293814659119,\n",
       " 0.5451378226280212,\n",
       " 0.6252576112747192,\n",
       " 0.54006427526474,\n",
       " 0.41269347071647644,\n",
       " 0.42440876364707947,\n",
       " 0.5576794147491455,\n",
       " 0.5382227897644043,\n",
       " 0.4888918399810791,\n",
       " 0.529583752155304,\n",
       " 0.2472391575574875,\n",
       " 0.6034968495368958,\n",
       " 0.5386080145835876,\n",
       " 0.42117419838905334,\n",
       " 0.587080180644989,\n",
       " 0.5679181814193726,\n",
       " 0.5497058629989624,\n",
       " 0.3362576365470886,\n",
       " 0.5918432474136353,\n",
       " 0.5909023880958557,\n",
       " 0.6529307961463928,\n",
       " 0.4904464781284332]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[-1] for x in incong_cosines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cff28804",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = flaubert_tokenizer(\"Le chat noir\", return_tensors=\"pt\")\n",
    "outputs = flaubert(**inputs)\n",
    "\n",
    "last2 = flaubert(**inputs)[0]\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85643056",
   "metadata": {},
   "source": [
    "embeds1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14434ee0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bb560aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states - last2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f335694a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0148e-01,  1.1297e+00,  1.8234e-01,  2.4061e-01, -3.8195e-01,\n",
       "          5.3658e-01, -1.9954e+00, -2.8238e-01,  1.2318e-01,  4.9372e-01,\n",
       "          4.2360e+00,  1.1093e+01, -2.3146e+00, -1.3214e-01,  2.5384e-02,\n",
       "         -2.9894e-01, -4.7218e-02,  1.1141e+00,  2.8397e+00, -1.5856e+00,\n",
       "         -9.8860e-02,  1.3485e+00, -9.5123e-01, -5.8757e-01,  2.1383e+00,\n",
       "          8.7860e-01,  1.3710e+00,  3.9834e-02, -3.1823e-01, -3.9053e-01,\n",
       "         -2.1020e-01, -4.7644e-01,  1.3672e+00,  2.2350e+00,  1.3919e+00,\n",
       "          2.2772e-01,  3.2120e-01, -2.9031e+00, -8.6726e-01, -2.9767e+00,\n",
       "         -3.5425e+00, -3.3491e-01, -4.8749e-01, -1.1013e+00, -9.3717e-01,\n",
       "         -1.4596e-01, -1.1507e+00,  5.2586e+00, -2.5021e+00, -9.7078e-01,\n",
       "          1.0084e+00,  2.6379e-01,  2.6935e-01,  2.6950e-02,  1.8818e+00,\n",
       "         -1.5453e+00,  2.0262e-01,  1.9667e+00,  1.9304e+00, -4.6436e-01,\n",
       "         -2.2892e+00,  2.8057e-01,  1.3692e+00, -1.0039e-01,  1.0707e+00,\n",
       "          1.3194e+00,  1.0814e+00, -1.1110e+00, -2.5053e+00, -3.2930e-01,\n",
       "         -2.6779e+00, -1.8757e+00, -8.3965e-01, -8.7202e-01,  1.5539e+00,\n",
       "          2.3902e+00,  8.4787e-01, -7.8020e-01, -1.3766e+00, -1.2985e+00,\n",
       "         -4.3159e+00, -4.2367e-01, -8.7025e-01,  8.5285e-01, -3.6064e+00,\n",
       "         -7.1396e-02, -1.8952e+00,  9.7531e-01,  1.9019e+00,  1.5426e+00,\n",
       "         -7.2571e-01,  2.5782e+00,  1.9013e+00, -2.3793e+00,  2.7673e+00,\n",
       "          6.2966e-01,  1.1097e+00, -7.1076e-01, -1.7429e+00, -5.4423e-01,\n",
       "          5.1857e-01, -2.2467e+00, -2.6127e-02, -1.8578e+00,  3.6994e-01,\n",
       "         -1.3240e+00, -1.5291e+00, -4.3356e-01,  1.2267e+00,  2.6952e+00,\n",
       "          7.0246e-01,  2.6045e+00, -4.8229e-01, -1.4594e-02, -1.0139e+00,\n",
       "          9.1805e-01, -6.9989e-01,  2.0668e+00,  3.3372e+00, -7.4285e-02,\n",
       "         -1.4964e-01,  6.7780e-01,  1.1492e+00,  5.5251e-01,  2.0147e-01,\n",
       "         -2.5323e+00, -8.0988e-01,  1.2532e-01, -1.0292e+00,  1.5990e+00,\n",
       "          1.7352e+00, -4.3675e-01,  1.9106e+00, -3.5029e-01, -1.5198e+00,\n",
       "          2.9327e+00, -3.0112e-01, -4.0464e-01, -2.0260e+00,  2.3290e+00,\n",
       "         -1.2650e-01, -8.2942e-01, -5.0969e-01, -4.3803e-01,  3.4837e+00,\n",
       "         -4.8125e-01,  1.4952e+00, -2.1182e-02,  3.5059e+00,  1.0487e+00,\n",
       "         -1.3970e+00, -1.1650e+00,  3.5902e-01, -1.1888e+00, -1.4687e+00,\n",
       "         -3.8017e+00, -1.6991e+00,  1.2919e+00,  3.7094e-01,  2.7561e-01,\n",
       "          2.3903e+00, -1.1848e+00, -5.1589e-02, -5.5743e-01, -7.4905e-01,\n",
       "         -1.2641e+00, -2.3351e+00, -1.2071e+00, -5.7289e-01,  1.3457e+00,\n",
       "          1.1565e+00,  2.5462e+00, -2.1090e+00, -2.2552e+00, -1.3628e+00,\n",
       "          2.8231e+00, -2.1179e+00, -1.7690e+00,  1.1786e+00, -1.0668e+00,\n",
       "          6.0245e-02, -3.7614e-01,  1.6308e+00, -4.3465e-01, -4.5189e-01,\n",
       "         -1.2213e+00,  6.2942e-02, -1.9221e+00, -1.3898e+00, -5.7929e-02,\n",
       "          1.5661e+00,  3.2986e-01,  2.2200e+00,  1.8278e+00,  1.1098e+00,\n",
       "         -3.0349e-01, -8.1496e-01,  6.1292e-01, -1.0169e+00, -4.7897e-01,\n",
       "         -8.1907e-01,  2.1475e+00,  1.1993e-01,  8.0268e-01,  3.0353e+00,\n",
       "         -1.1227e+00,  5.4817e-03,  7.0121e-01,  2.0075e-01, -1.5283e+00,\n",
       "          2.4137e+00, -3.2087e-01,  2.0846e+00, -1.8002e+00,  2.7528e+00,\n",
       "         -8.3183e-01,  6.8415e-01,  5.3898e-01,  1.9832e+00, -4.0285e-01,\n",
       "         -7.0248e-01,  2.8207e+00,  5.5182e-01, -6.5558e-01,  1.1415e+00,\n",
       "         -1.1949e+00,  4.0255e-01,  2.3440e+00, -4.7353e-01, -3.3860e+00,\n",
       "          5.7658e-02,  1.2542e-01, -1.6154e+00, -1.0251e+00,  2.1425e+00,\n",
       "         -9.5924e-01, -2.0475e+00, -1.6539e-01, -2.6949e-01,  1.2511e+00,\n",
       "          7.8095e-01, -1.2436e+00,  2.7567e+00,  2.2153e+00, -6.2005e-01,\n",
       "         -2.7080e+00,  1.1833e+00,  1.8302e-01,  5.8932e-01,  4.9641e-01,\n",
       "         -1.5591e+00, -1.0712e+00,  1.6900e+00, -1.8883e+00,  3.1129e-01,\n",
       "         -1.5610e+00, -3.5555e+00,  6.7005e-01,  2.4451e+00, -6.0494e-01,\n",
       "          1.1824e+00, -1.0344e+00,  1.3114e+00,  1.6110e-01,  9.6353e-01,\n",
       "          1.0652e-01,  1.5011e+00, -2.8845e+00, -8.3774e-01,  2.1055e-02,\n",
       "          5.5721e-01, -1.6492e-01, -2.3698e+00, -1.6240e+00, -3.5870e+00,\n",
       "          8.5114e-01, -8.6588e-01, -2.0330e+00,  3.1814e+00, -7.0285e-01,\n",
       "          5.5138e-01, -9.2847e-01, -9.3655e-01,  1.6415e+00,  8.8012e-01,\n",
       "          2.7396e+00, -4.2970e-01, -1.8053e+00, -1.3401e+00, -1.7717e+00,\n",
       "         -1.9919e+00, -1.4286e+00, -1.3308e+00,  3.3644e-01,  2.9733e+00,\n",
       "          3.4343e+00, -8.6793e-01, -1.0303e-01, -1.3667e+00,  2.4739e+00,\n",
       "         -1.3273e+00, -6.1368e-01,  8.2935e-01,  8.9641e-01,  1.3627e+00,\n",
       "          1.6059e+00, -1.0022e+00,  1.8511e+00, -7.3873e-01, -1.0616e-02,\n",
       "          1.9896e+00,  1.5263e+00,  1.7215e+00, -2.6658e+00, -6.8493e-01,\n",
       "          1.0267e+00, -4.3999e+00, -6.0955e-02, -3.4565e-01,  3.1124e-01,\n",
       "          1.9899e+00,  1.7135e+00, -4.0479e-01,  1.0155e+00, -1.5812e+00,\n",
       "          1.9255e+00, -1.7607e+00, -3.1089e-01, -1.5359e+00,  9.8413e-01,\n",
       "         -1.6789e-01, -1.0696e+00, -9.8428e-01,  4.5101e-01, -1.1963e+00,\n",
       "         -2.0775e-01,  1.0064e+00, -2.3296e+00, -4.0495e-01,  1.1935e+00,\n",
       "          9.4621e-01, -5.8240e-01,  1.8143e+00,  3.2605e-01,  4.0094e-01,\n",
       "         -2.0849e+00, -3.7124e+00,  1.9703e-01,  1.6688e+00,  4.1901e+00,\n",
       "          5.1629e-01, -1.3363e+00,  1.9085e-01, -7.2992e-01,  2.2113e+00,\n",
       "         -9.4020e-01,  8.7432e-01,  1.6849e+00,  5.2165e-01, -1.8372e+00,\n",
       "         -1.3505e+00,  1.4344e-01, -1.3226e-01, -5.5241e-01,  2.1867e-01,\n",
       "          1.2388e+00, -1.3180e-01,  6.9821e-01, -1.5769e+00,  3.0660e-01,\n",
       "         -1.1358e+00, -1.0163e+00, -1.3041e-01,  2.9658e+00,  1.8282e+00,\n",
       "          1.1876e+00,  3.7725e+00,  2.4312e-01, -1.7750e+00, -5.1290e+00,\n",
       "          1.3712e+00,  1.8921e-01, -8.6571e-02,  1.8191e-01,  3.2780e-01,\n",
       "          5.7697e-01,  1.9751e+00,  6.0613e-01,  3.2215e+00,  2.9808e+00,\n",
       "          5.4550e-01, -1.7278e+00, -2.1420e+00, -1.3487e+00, -7.3064e-01,\n",
       "          4.8265e-01,  1.1559e+00, -7.3719e-01,  1.7512e+00, -2.7097e+00,\n",
       "         -1.5555e+00, -2.7546e+00, -7.1967e-02, -5.4862e-01, -2.2855e-01,\n",
       "          9.5241e-01,  1.2475e-01,  5.8819e-01,  4.6761e-01,  4.3329e-01,\n",
       "          1.3205e+00,  1.8009e+00, -8.3407e-01, -2.1768e+00, -5.0158e-01,\n",
       "          3.8096e-01, -9.7117e-02,  2.4064e+00, -1.6003e+00, -3.2002e-01,\n",
       "         -8.6677e-01, -3.7327e+00, -1.4827e+00, -9.1993e-02, -1.1288e+00,\n",
       "          3.3415e-03, -4.2505e-01, -3.3827e-01, -2.3172e+00, -5.1666e-02,\n",
       "         -2.0855e+00,  1.3474e+00,  8.2265e-01, -3.6241e+00,  1.9859e+00,\n",
       "         -1.6918e+00, -3.6913e-01, -4.2894e-01, -1.4466e+00,  1.5173e+00,\n",
       "          9.5644e-01,  9.6156e-01,  3.8418e-01,  5.5815e-02,  2.6049e+00,\n",
       "         -4.5774e-01, -6.6799e-01,  1.2257e+00,  7.9247e-01,  6.8491e-01,\n",
       "         -1.0545e+00, -5.9743e-01, -5.2978e-02, -1.3526e+00, -1.3320e+00,\n",
       "         -7.4230e-01, -4.6015e-01,  1.9037e+00,  8.1512e-01, -1.3875e-01,\n",
       "         -1.2605e+00, -4.3727e-01, -4.2198e-01, -2.8732e-01,  8.1612e-01,\n",
       "         -3.9904e-01, -3.9168e-01, -8.2611e-01,  3.4522e+00, -7.5238e-01,\n",
       "         -1.1267e+00,  7.2217e-02,  9.0581e-01, -1.0941e+00, -6.1950e-01,\n",
       "          8.7779e-01,  6.6446e-01, -8.7244e-01, -1.6739e-01,  2.4789e+00,\n",
       "         -2.6576e-01, -5.4407e-01,  1.1300e-01, -1.8510e+00,  1.0762e-01,\n",
       "          1.4724e+00,  5.3445e-01,  1.1106e+00, -1.0525e-01, -1.7654e+00,\n",
       "         -2.6335e-01, -2.1955e+00, -6.7454e-03,  9.5594e-02,  3.0697e+00,\n",
       "          1.4561e+00,  1.8553e-01,  1.7473e+00,  1.6740e+00, -8.7778e-02,\n",
       "          9.0883e-01, -5.4101e-01,  8.1677e-01,  9.3211e-01, -6.7335e-01,\n",
       "         -1.4315e+00, -1.7477e+00,  1.5831e+00, -5.5089e-01, -3.5778e-01,\n",
       "          1.5373e+00,  1.3665e+00, -3.2290e-01,  3.0610e+00, -2.6581e+00,\n",
       "          1.3039e+00, -1.9569e+00,  1.0403e+00, -2.1472e-02, -1.9073e+00,\n",
       "         -1.2913e+00,  1.3045e+00,  3.1700e-01, -2.6400e-01, -1.3382e+00,\n",
       "          3.7365e+00, -8.9547e-01, -1.7926e-01, -3.0985e-01,  4.1096e-02,\n",
       "         -3.3835e-01,  9.9380e-02,  8.3590e-01,  6.9345e-01,  2.3952e+00,\n",
       "         -9.7130e-02, -1.8486e+00,  5.9436e-01, -3.3718e+00,  1.7169e+00,\n",
       "         -8.9800e-02,  9.2409e-02, -8.8258e-01,  9.8023e-03, -8.8052e-03,\n",
       "          1.2121e+00, -1.4697e+00,  8.1373e-01, -1.5640e+00,  3.5318e-02,\n",
       "          5.2549e-03, -3.5473e-01,  1.1756e+00,  1.2534e+00,  1.1442e+00,\n",
       "          9.8051e-01, -8.7219e-01,  5.4537e-02,  8.5489e-01,  4.0330e+00,\n",
       "          2.5523e+00,  1.3900e+00, -9.3549e-01,  1.8454e-01, -7.0096e-01,\n",
       "          2.1828e+00, -5.3717e-01,  8.6470e-01, -9.6895e-01, -2.0838e+00,\n",
       "          8.3836e-01,  2.1368e+00, -1.7662e+00,  2.3000e+00,  3.5090e+00,\n",
       "         -4.1701e-01, -4.3417e-01,  4.9821e-01,  6.7413e-01, -1.0142e+00,\n",
       "          2.5868e+00, -1.9077e+00,  2.0253e-01, -7.1681e-01,  1.5821e+00,\n",
       "         -4.9553e-01,  8.5350e-01, -1.1648e+00,  2.1082e+00, -2.3258e+00,\n",
       "         -8.0733e-01,  1.0917e+00, -1.9492e+00, -1.8716e+00,  1.1718e+00,\n",
       "          2.8250e-01, -1.8033e-01,  3.3791e+00, -2.8532e+00, -3.0763e+00,\n",
       "         -1.1712e+00, -3.4004e-01,  3.3875e-01, -4.0085e-02, -4.1649e-01,\n",
       "         -3.0282e-02,  2.2204e+00, -2.7390e-02,  1.0785e+00,  2.1064e+00,\n",
       "          4.6829e-01,  2.3597e+00,  7.0506e-01,  7.5688e-01,  3.1606e+00,\n",
       "          9.1641e-02,  1.1127e+00, -1.7671e+00,  7.3569e-01, -1.2232e+00,\n",
       "          4.3815e-01,  6.6466e-01, -3.2989e-01,  5.6650e-01, -1.1248e+00,\n",
       "          2.7002e-01,  2.1486e+00, -1.6105e+00, -2.5878e-01,  9.2731e-01,\n",
       "         -8.0711e-01,  5.4490e-01, -8.5470e-01, -1.0002e+00,  1.1821e+00,\n",
       "         -2.4022e+00,  1.6705e+00, -1.4380e+00,  2.9144e+00, -6.3192e-01,\n",
       "         -2.3336e-01, -1.0417e+00, -1.0213e+00,  2.7875e+00,  1.5783e+00,\n",
       "         -2.5984e+00,  6.5368e-01,  4.6254e+00, -2.8068e+00, -1.0376e+00,\n",
       "          2.0203e-01,  1.1907e+00,  1.7874e+00, -1.4237e+00,  7.0976e-01,\n",
       "          2.1346e-01,  1.8128e+00,  7.7245e-01,  1.1437e+00, -2.3103e+00,\n",
       "         -2.4559e+00,  1.2967e+00, -1.1414e+00, -4.0380e-01,  1.7058e-01,\n",
       "         -3.6960e-01, -2.1090e+00,  1.4964e-01,  3.3253e+00, -1.9819e+00,\n",
       "          1.7646e+00,  5.0237e-01, -1.4213e+00,  1.2438e+00, -1.3779e+00,\n",
       "         -1.7686e+00,  7.2897e-01,  6.1900e-01,  2.8143e+00, -1.6092e-02,\n",
       "          1.2421e+00,  9.9138e-01,  1.8039e+00, -2.6319e-01, -1.2564e+00,\n",
       "          1.1343e+00,  8.3152e-01,  3.4393e+00,  7.7030e-01,  1.6413e+00,\n",
       "         -8.8346e-01, -2.8509e+00, -3.2827e-01, -2.8976e-01,  1.1148e+00,\n",
       "          7.3664e-01,  2.6141e+00,  2.4352e+00, -3.2828e-01, -6.5332e-01,\n",
       "          1.3169e+00, -2.2849e+00, -6.4815e-01,  8.7118e-01,  1.9089e-01,\n",
       "         -9.1181e-01,  1.2214e+00,  2.1619e+00,  1.3012e+00, -8.4944e-01,\n",
       "          1.4337e+00, -1.4093e+00,  1.9658e+00,  2.2280e+00,  5.9673e-01,\n",
       "         -6.7056e-01, -1.2556e+00, -6.0223e-01, -1.5493e+00,  9.0935e-01,\n",
       "          1.6245e+00,  1.6960e+00,  6.3717e-01, -2.5647e+00,  9.7382e-01,\n",
       "         -3.0842e-01, -4.3272e-02, -1.7229e-01,  9.3449e-02, -9.4498e-01,\n",
       "         -2.9848e+00,  3.7210e-01,  3.3563e+00, -3.0093e+00, -1.8548e-01,\n",
       "          2.1265e-02, -2.5331e+00,  1.3427e+00,  1.2930e-01, -3.5616e-01,\n",
       "         -1.6568e+00,  3.5621e-01,  6.6075e-01, -3.1452e+00, -3.2358e-01,\n",
       "         -1.0013e-01, -1.8027e+00, -1.4403e+00,  6.5230e-01, -1.9217e+00,\n",
       "         -2.2487e+00, -3.7129e-01,  2.9409e+00,  8.9906e-02,  4.2940e-01,\n",
       "          1.4688e+00,  1.8090e+00,  9.1474e-01, -9.4904e-02, -2.5755e-01,\n",
       "          2.9361e-01,  9.9604e-01,  1.0841e+00,  1.0484e+00,  3.8931e-01,\n",
       "         -1.6865e+00, -7.9174e-02, -1.2422e+00]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds2[:,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ec99a4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos(embeds1[:,2,:],embeds1[:,2,:]).asDou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6ed21523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds1.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee1d8aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Pour',\n",
       "  'accrocher',\n",
       "  'le',\n",
       "  'cadre,',\n",
       "  'Nicholas',\n",
       "  'avait',\n",
       "  'besoin',\n",
       "  'd’un',\n",
       "  'marteau',\n",
       "  'et',\n",
       "  'd’un',\n",
       "  'clou'],\n",
       " ['Dans',\n",
       "  'la',\n",
       "  'douche,',\n",
       "  'il',\n",
       "  'se',\n",
       "  'lave',\n",
       "  'la',\n",
       "  'peau',\n",
       "  'avec',\n",
       "  'du',\n",
       "  'savon'],\n",
       " ['Elle',\n",
       "  'est',\n",
       "  'allée',\n",
       "  'à',\n",
       "  'la',\n",
       "  'boulangerie',\n",
       "  'pour',\n",
       "  'acheter',\n",
       "  'une',\n",
       "  'miche',\n",
       "  'de',\n",
       "  'pain'],\n",
       " ['Katie', 'a', 'mis', 'le', 'bouquet', 'de', 'fleurs', 'dans', 'un', 'vase'],\n",
       " ['Le', 'cowboy', 'a', 'mis', 'la', 'selle', 'sur', 'le', 'cheval'],\n",
       " ['Le', 'fermier', 'ne', 'veut', 'pas', 'traire', 'sa', 'vache'],\n",
       " ['La', 'nuit,', 'Anik', 'verrouille', 'la', 'porte'],\n",
       " ['L’enfant', 'se', 'couche', 'dans', 'le', 'lit'],\n",
       " ['Au', 'parc,', 'la', 'fille', 'se', 'balance', 'sur', 'la', 'balançoire'],\n",
       " ['Autour', 'de', 'la', 'taille,', 'il', 'porte', 'une', 'ceinture'],\n",
       " ['Chloé', 'boit', 'son', 'thé', 'glacé', 'avec', 'une', 'paille'],\n",
       " ['Ils', 'sont', 'allés', 'voir', 'le', 'nouveau', 'film', 'au', 'cinéma'],\n",
       " ['La', 'petite', 'fille', 's’est', 'couchée', 'dans', 'son', 'lit'],\n",
       " ['Les', 'oisillons', 'sont', 'prêts', 'à', 'partir', 'du', 'nid'],\n",
       " ['L’enseignante',\n",
       "  'de',\n",
       "  'mathématique',\n",
       "  'a',\n",
       "  'écrit',\n",
       "  'le',\n",
       "  'problème',\n",
       "  'sur',\n",
       "  'le',\n",
       "  'tableau'],\n",
       " ['Ce', 'matin', 'le', 'professeur', 'a', 'bu', 'un', 'café'],\n",
       " ['Claude', 'voit', 'une', 'annonce', 'dans', 'le', 'journal'],\n",
       " ['Suzanne', 'mange', 'un', 'cornet', 'de', 'crème'],\n",
       " ['Ils', 'sont', 'assis', 'ensemble', 'sans', 'dire', 'un', 'seul', 'mot'],\n",
       " ['Le', 'chaton', 'joue', 'avec', 'la', 'pelote', 'de', 'laine'],\n",
       " ['Alice',\n",
       "  'va',\n",
       "  'à',\n",
       "  'la',\n",
       "  'bibliothèque',\n",
       "  'pour',\n",
       "  'emprunter',\n",
       "  'un',\n",
       "  'livre'],\n",
       " ['Jean', 'a', 'pêché', 'un', 'gros', 'poisson'],\n",
       " ['Claire',\n",
       "  'a',\n",
       "  'envoyé',\n",
       "  'la',\n",
       "  'carte',\n",
       "  'd’anniversaire',\n",
       "  'par',\n",
       "  'la',\n",
       "  'poste'],\n",
       " ['L’ambulance', 'approche,', 'j’entends', 'la', 'sirène'],\n",
       " ['Papa', 'a', 'tranché', 'la', 'dinde', 'avec', 'un', 'couteau'],\n",
       " ['Elle', 'coupe', 'les', 'légumes', 'sur', 'le', 'comptoir'],\n",
       " ['Ils', 'ont', 'pris', 'le', 'pont', 'pour', 'traverser', 'la', 'rivière'],\n",
       " ['Les',\n",
       "  'abeilles',\n",
       "  'récoltent',\n",
       "  'du',\n",
       "  'pollen',\n",
       "  'pour',\n",
       "  'faire',\n",
       "  'du',\n",
       "  'miel'],\n",
       " ['L’enseignant', 'lui', 'rappelle', 'de', 'faire', 'son', 'devoir'],\n",
       " ['Ma',\n",
       "  'tondeuse',\n",
       "  'est',\n",
       "  'brisée,',\n",
       "  'je',\n",
       "  'ne',\n",
       "  'peux',\n",
       "  'pas',\n",
       "  'tondre',\n",
       "  'le',\n",
       "  'gazon'],\n",
       " ['Annie',\n",
       "  'a',\n",
       "  'pris',\n",
       "  'un',\n",
       "  'mouchoir',\n",
       "  'pour',\n",
       "  'se',\n",
       "  'moucher',\n",
       "  'le',\n",
       "  'nez'],\n",
       " ['La', 'professeure', 'a', 'aiguisé', 'son', 'crayon'],\n",
       " ['Il', 'a', 'envoyé', 'la', 'lettre', 'sans', 'un', 'timbre'],\n",
       " ['Charles', 'se', 'brosse', 'les', 'cheveux', 'avec', 'la', 'brosse'],\n",
       " ['Ce', 'garçon', 'lit', 'beaucoup', 'de', 'livres'],\n",
       " ['Éloi', 'allume', 'sa', 'pipe', 'avec', 'une', 'allumette'],\n",
       " ['Le', 'chanteur', 'a', 'chanté', 'une', 'belle', 'chanson'],\n",
       " ['Le', 'poète', 'a', 'écrit', 'un', 'poème'],\n",
       " ['Pour',\n",
       "  'protéger',\n",
       "  'sa',\n",
       "  'tête',\n",
       "  'du',\n",
       "  'soleil,',\n",
       "  'Nicole',\n",
       "  'porte',\n",
       "  'un',\n",
       "  'chapeau'],\n",
       " ['Luc', 'verse', 'le', 'jus', 'dans', 'un', 'verre']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb50b1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04248666763305664,\n",
       " 0.059577006846666336,\n",
       " 0.5303464531898499,\n",
       " 0.8240672945976257,\n",
       " 0.8373686075210571,\n",
       " 0.7211694717407227,\n",
       " 0.17833147943019867,\n",
       " 0.7575597167015076,\n",
       " 0.680794358253479,\n",
       " 0.7156547904014587,\n",
       " 0.5590625405311584]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cosines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "89c5627f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04248666763305664,\n",
       " 0.059577006846666336,\n",
       " 0.5303464531898499,\n",
       " 0.8240672945976257,\n",
       " 0.8373686075210571,\n",
       " 0.7211694717407227,\n",
       " 0.17833147943019867,\n",
       " 0.7575597167015076,\n",
       " 0.680794358253479,\n",
       " 0.7156547904014587,\n",
       " 0.1826620250940323]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cosines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "494773cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mean() received an invalid combination of arguments - got (out=NoneType, axis=int, dtype=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-30c0e0d3fe08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3415\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3417\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3419\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[0;31mTypeError\u001b[0m: mean() received an invalid combination of arguments - got (out=NoneType, axis=int, dtype=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "np.mean(b,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8a79ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[9.6767e-01, 9.7584e-01, 5.4985e-01,  ..., 4.4432e-01,\n",
       "          7.7414e-01, 8.6185e-01],\n",
       "         [5.2245e-01, 2.8586e-01, 7.5445e-01,  ..., 8.9782e-02,\n",
       "          3.6117e-01, 9.3918e-01],\n",
       "         [4.4737e-01, 6.7895e-01, 5.1279e-02,  ..., 6.1994e-02,\n",
       "          2.5984e-01, 6.6802e-02],\n",
       "         ...,\n",
       "         [7.5941e-01, 1.6501e-01, 9.3603e-01,  ..., 9.7927e-01,\n",
       "          5.9023e-01, 9.2573e-01],\n",
       "         [6.8829e-01, 3.2672e-01, 2.5859e-01,  ..., 9.0287e-01,\n",
       "          9.1071e-01, 4.7068e-01],\n",
       "         [3.1619e-01, 2.2824e-01, 4.1589e-01,  ..., 1.5110e-01,\n",
       "          2.9937e-01, 4.7298e-01]],\n",
       "\n",
       "        [[9.8613e-01, 8.4673e-01, 1.4627e-01,  ..., 7.6809e-01,\n",
       "          4.5921e-01, 4.1852e-01],\n",
       "         [3.9211e-01, 2.3385e-01, 2.0254e-01,  ..., 7.4772e-01,\n",
       "          3.2448e-01, 4.2425e-01],\n",
       "         [3.4798e-01, 1.7078e-01, 4.1112e-01,  ..., 9.1190e-01,\n",
       "          6.1649e-01, 2.2077e-01],\n",
       "         ...,\n",
       "         [8.7191e-01, 1.7095e-02, 1.5932e-01,  ..., 9.4920e-01,\n",
       "          1.7329e-01, 6.9124e-01],\n",
       "         [4.7104e-01, 8.8113e-01, 7.0605e-01,  ..., 4.4510e-01,\n",
       "          3.7783e-01, 1.0409e-01],\n",
       "         [5.6925e-01, 4.5762e-01, 7.9396e-01,  ..., 6.1270e-01,\n",
       "          8.5939e-01, 5.2411e-01]],\n",
       "\n",
       "        [[9.9310e-01, 3.9137e-01, 9.0431e-01,  ..., 5.7176e-01,\n",
       "          6.5175e-01, 5.7785e-01],\n",
       "         [4.6392e-01, 7.3583e-01, 8.2742e-01,  ..., 9.3254e-03,\n",
       "          8.5763e-01, 7.4605e-01],\n",
       "         [7.4290e-01, 5.7289e-01, 5.5889e-01,  ..., 9.3557e-01,\n",
       "          9.1904e-04, 3.2760e-02],\n",
       "         ...,\n",
       "         [6.0589e-01, 7.8055e-01, 9.1811e-01,  ..., 6.2187e-02,\n",
       "          7.3213e-01, 4.5146e-01],\n",
       "         [8.8075e-01, 3.0550e-01, 1.6241e-02,  ..., 3.4369e-01,\n",
       "          8.5162e-01, 9.9567e-02],\n",
       "         [9.3276e-02, 8.9333e-01, 8.9732e-01,  ..., 9.4702e-01,\n",
       "          4.1685e-01, 8.1823e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[2.9397e-01, 5.2113e-01, 3.8236e-01,  ..., 8.4754e-01,\n",
       "          1.2827e-01, 1.8965e-01],\n",
       "         [7.9158e-01, 2.3455e-01, 2.9426e-01,  ..., 7.7643e-01,\n",
       "          6.3663e-01, 4.9053e-01],\n",
       "         [1.2748e-01, 5.9511e-02, 5.8772e-01,  ..., 5.7432e-01,\n",
       "          1.5638e-01, 7.4016e-01],\n",
       "         ...,\n",
       "         [3.7278e-01, 1.8795e-01, 6.6595e-01,  ..., 5.5735e-01,\n",
       "          7.3936e-01, 3.3476e-01],\n",
       "         [6.6780e-01, 4.9891e-01, 9.6636e-01,  ..., 1.3694e-01,\n",
       "          6.9212e-01, 7.9692e-02],\n",
       "         [3.8371e-01, 5.9945e-01, 1.8555e-01,  ..., 2.6157e-02,\n",
       "          3.4894e-01, 4.7977e-01]],\n",
       "\n",
       "        [[2.6040e-01, 4.3737e-01, 7.9503e-01,  ..., 2.1957e-02,\n",
       "          2.8460e-01, 1.9633e-01],\n",
       "         [5.0998e-01, 4.2167e-01, 1.4473e-01,  ..., 8.3559e-03,\n",
       "          1.8586e-01, 3.8447e-01],\n",
       "         [5.9918e-01, 2.3122e-01, 4.1041e-01,  ..., 5.1590e-01,\n",
       "          1.3591e-01, 7.5521e-01],\n",
       "         ...,\n",
       "         [7.7683e-01, 3.0364e-01, 3.5876e-01,  ..., 6.6922e-01,\n",
       "          8.6517e-01, 7.1610e-01],\n",
       "         [9.0339e-01, 2.1961e-01, 3.1546e-01,  ..., 7.3241e-01,\n",
       "          3.0359e-01, 8.1773e-01],\n",
       "         [2.7261e-01, 5.0420e-01, 3.9468e-01,  ..., 4.8497e-01,\n",
       "          5.9136e-01, 8.8607e-01]],\n",
       "\n",
       "        [[7.2488e-01, 1.0930e-01, 1.3255e-01,  ..., 1.7393e-01,\n",
       "          3.0538e-01, 2.8941e-01],\n",
       "         [3.2697e-01, 4.6876e-01, 1.0565e-01,  ..., 5.6247e-01,\n",
       "          1.0223e-01, 7.9847e-01],\n",
       "         [5.7114e-01, 3.8433e-01, 9.5090e-01,  ..., 5.7549e-01,\n",
       "          6.1081e-01, 2.6310e-01],\n",
       "         ...,\n",
       "         [3.0598e-01, 9.3411e-01, 9.4346e-01,  ..., 1.2722e-02,\n",
       "          1.4236e-01, 1.8182e-02],\n",
       "         [7.4404e-01, 3.9543e-01, 8.3360e-01,  ..., 5.0919e-01,\n",
       "          6.2316e-01, 3.1329e-01],\n",
       "         [2.0816e-01, 6.8282e-01, 6.7093e-01,  ..., 9.8876e-02,\n",
       "          1.0568e-01, 7.5464e-01]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "for i in range(100):\n",
    "    a.append(torch.rand(1, 100, 100))\n",
    "\n",
    "b = torch.Tensor(100, 100, 100)\n",
    "torch.cat(a, out=b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d834cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Pour  <special1> ,\"\n",
    "tokens = torch.tensor([flaubert_tokenizer.encode(sentence)]) # get tokens from FlauBERT\n",
    "all_vectors = flaubert(tokens)[0] # get embedding vectors from FlauBERT\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8500f4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4911e+00, -2.4542e-02,  1.0891e+00, -7.0925e-01,  5.7965e-01,\n",
       "         -1.2396e+00, -1.0774e+00, -5.6120e-01, -7.6051e-01,  7.9824e-01,\n",
       "          1.0770e+00,  3.9578e+00, -1.2487e+00,  9.0368e-02, -1.3674e+00,\n",
       "          1.0787e+00,  6.8829e-01, -2.3093e+00,  3.2172e-01,  7.6298e-01,\n",
       "          1.0884e+00,  1.6345e+00,  1.9358e+00,  1.7736e-02, -1.0347e-01,\n",
       "          1.1699e+00,  2.3415e+00, -3.8385e-01, -8.8784e-02,  2.0283e+00,\n",
       "          1.6015e+00, -3.3875e-01,  2.3784e+00,  2.0865e+00, -7.3681e-01,\n",
       "          1.3047e+00,  1.6593e+00,  1.5166e-01, -1.1779e-01, -4.0302e-01,\n",
       "         -1.1826e+00,  2.1468e+00, -1.4707e+00, -2.1167e+00, -1.9920e+00,\n",
       "          2.6255e-01, -5.0288e-02,  2.0467e+00, -1.5069e+00, -2.0714e+00,\n",
       "          1.0657e+00,  7.1954e-03,  1.0887e+00,  8.9785e-01, -6.2422e-01,\n",
       "         -6.9802e-01,  5.1493e-01,  5.2547e-01,  1.1255e+00,  1.0847e-02,\n",
       "         -1.8135e+00,  4.8970e-01, -7.2585e-01,  1.9124e-01,  5.3030e-01,\n",
       "         -1.7383e-01,  1.3756e+00,  1.4631e+00, -2.6636e-01,  6.0119e-01,\n",
       "         -1.5217e+00, -4.7936e-01, -2.6126e-01, -7.3291e-01,  1.6302e+00,\n",
       "          2.4626e+00,  6.5039e-02,  1.1001e+00, -2.3904e+00, -4.8836e-01,\n",
       "          2.5995e-01,  1.1983e+00, -8.4794e-01, -1.3920e+00, -3.8985e-01,\n",
       "          3.7409e-01,  7.7038e-01, -1.2195e+00,  1.5920e+00, -3.9570e-01,\n",
       "         -3.8877e-01,  4.0297e-01, -4.5215e-01,  2.2983e-01,  3.3532e-01,\n",
       "         -5.4754e-02,  1.1370e+00, -1.0197e+00, -3.6260e-01, -5.5420e-01,\n",
       "         -5.3715e-01,  7.4830e-01,  1.1598e+00,  6.8544e-01, -1.3200e+00,\n",
       "          3.2196e-01, -1.1066e+00,  6.0745e-01,  1.2269e+00, -4.2379e-01,\n",
       "          2.2660e-01,  3.9156e-01, -1.5474e-01,  5.1830e-01,  6.4045e-02,\n",
       "         -9.1295e-01,  4.8198e-01, -8.9739e-02,  2.5713e+00,  6.0542e-01,\n",
       "         -2.0807e-01,  1.3277e+00,  1.4676e-01,  2.5012e+00,  2.3910e-01,\n",
       "         -1.6423e-01,  4.8341e-01,  2.0715e+00, -9.6690e-01,  2.0583e+00,\n",
       "          3.8827e-01,  1.8429e+00,  1.6478e+00,  1.3420e-01, -1.8354e+00,\n",
       "          2.2210e-01, -9.0897e-01, -2.6807e-01, -8.2216e-01, -5.3421e-01,\n",
       "         -1.5816e+00,  1.5714e+00, -1.1644e+00,  1.2219e+00,  5.0379e-01,\n",
       "         -1.3364e+00,  1.7570e+00,  6.3014e-01,  4.1555e+00,  1.0789e+00,\n",
       "          5.5576e-01,  1.1418e-01, -3.1276e-01, -2.5441e+00, -6.6347e-01,\n",
       "         -2.2323e+00, -1.6011e+00, -3.5447e-01, -9.6730e-01, -8.8063e-01,\n",
       "          7.0207e-01, -1.0168e+00,  9.1727e-03, -1.1393e+00, -6.7638e-01,\n",
       "         -6.3006e-01,  7.9199e-01,  2.3025e-01, -1.4058e-01, -7.4721e-01,\n",
       "         -9.5168e-01,  2.7749e+00, -2.4726e-04, -1.8752e+00, -5.5285e-01,\n",
       "          2.5874e-02,  3.5660e-01, -7.9521e-02,  1.2018e+00, -1.8548e+00,\n",
       "         -4.7885e-01, -2.3732e+00,  2.3823e+00, -5.0002e-01,  1.1978e+00,\n",
       "          1.1512e+00,  1.4909e-01,  8.0847e-01, -1.7660e+00,  1.0455e+00,\n",
       "         -1.1726e+00, -6.0170e-01,  2.2862e-01,  2.2633e+00,  2.4181e+00,\n",
       "          5.5943e-01, -6.9166e-01,  3.5263e-01,  9.2355e-02, -5.4195e-01,\n",
       "          1.9176e+00,  2.8879e-01,  9.9979e-01, -8.6915e-01,  1.5935e+00,\n",
       "         -1.6740e-01,  1.0248e+00,  2.2114e-01,  1.2994e+00, -2.1785e+00,\n",
       "         -2.5253e-01,  5.3915e-01, -2.8652e-02, -2.1049e+00, -7.2610e-01,\n",
       "         -4.5234e-01,  5.1381e-01, -3.4189e-01, -3.5775e-01, -1.1495e+00,\n",
       "         -9.7848e-01, -2.0577e-01,  4.8306e-02,  4.1040e-02, -1.3443e+00,\n",
       "         -6.3303e-01, -5.0693e-01,  2.7722e-01,  1.6438e-01,  1.1979e+00,\n",
       "         -1.1946e-01,  4.1158e-01, -1.0449e+00,  7.8461e-01,  2.1209e+00,\n",
       "          1.8772e-01,  2.1040e+00,  1.4780e+00, -7.9460e-01,  1.4019e+00,\n",
       "         -3.9977e-01,  1.0891e-01, -4.6119e-02,  2.1359e+00, -5.9431e-01,\n",
       "         -2.8086e+00, -2.4251e+00,  3.4025e-01, -2.7743e-01,  3.1546e-01,\n",
       "         -2.1719e+00,  5.4101e-02, -7.4284e-01, -7.6493e-01, -6.2091e-01,\n",
       "         -9.6884e-01, -1.6119e+00,  7.6478e-02,  8.2916e-01,  1.9048e+00,\n",
       "          7.4331e-01, -3.4172e-01,  1.8186e+00,  1.8428e+00, -1.5402e+00,\n",
       "          6.6134e-01,  8.0258e-02,  9.8712e-01, -9.5193e-01,  1.6460e+00,\n",
       "         -1.2306e+00,  1.1416e+00,  2.9810e-01,  1.2623e-01, -9.5988e-01,\n",
       "          1.0086e+00,  6.7713e-01, -8.6737e-01,  3.0501e+00,  5.6634e-01,\n",
       "          1.6593e-01, -1.1142e+00, -3.5113e-01, -1.1631e-01,  5.9209e-01,\n",
       "         -1.7744e+00, -9.4223e-02, -7.5399e-01, -3.3398e-01, -1.6022e-01,\n",
       "          4.4055e-01, -3.1422e-01,  6.9431e-01,  7.1389e-01,  8.6047e-01,\n",
       "          8.8804e-01,  2.2110e+00,  3.5127e-01, -1.0071e+00,  1.4369e+00,\n",
       "         -1.0765e+00,  7.6382e-02,  4.2845e-01, -6.3158e-01, -2.3066e-01,\n",
       "          4.0049e-01, -2.2518e-01,  1.0735e+00, -1.0663e+00,  4.3562e-01,\n",
       "          1.3874e+00,  4.9207e-01, -5.3724e-01, -8.9132e-01,  6.3970e-01,\n",
       "         -2.4714e+00, -2.0486e+00,  1.0839e+00,  3.7480e-01, -9.3204e-01,\n",
       "         -1.4163e+00,  1.6426e+00,  3.3760e-02, -9.1163e-01, -8.0140e-01,\n",
       "          1.2523e+00,  3.5703e-01, -1.2856e+00, -1.2938e+00,  3.5379e-01,\n",
       "         -8.5890e-02,  1.7587e-01, -5.1590e-01, -4.8560e-01,  3.8603e-01,\n",
       "         -7.8092e-01, -1.2211e+00,  6.9693e-01,  9.7630e-01, -3.2163e-02,\n",
       "          8.2644e-01,  1.0529e+00, -7.7618e-01,  4.8277e-01,  2.9051e-01,\n",
       "          5.0950e-01, -1.3158e+00,  1.2531e-01,  1.3763e+00,  2.2054e+00,\n",
       "          1.4009e+00,  6.3190e-01, -4.4102e-01, -1.8273e-01,  2.0246e+00,\n",
       "          8.2180e-01,  1.3872e+00,  2.2524e+00, -5.9530e-01, -1.1307e+00,\n",
       "         -3.4306e-01, -6.5332e-01,  1.1556e+00, -4.7430e-01,  7.6822e-01,\n",
       "          4.7243e-01, -2.0278e+00, -9.4913e-01,  4.6358e-01,  1.6653e+00,\n",
       "         -5.1312e-01, -1.1581e+00,  1.9952e-02, -2.5285e+00,  2.6760e+00,\n",
       "         -3.0533e-01,  8.1939e-02,  9.1242e-01,  1.9183e-01, -3.2288e-01,\n",
       "          1.3023e+00,  1.3923e+00,  3.2647e-01, -2.2984e-01,  6.7290e-01,\n",
       "          9.2124e-01,  9.7773e-01, -1.4015e+00,  6.9365e-02, -2.1234e+00,\n",
       "          1.6347e-01, -2.0409e+00, -1.4276e+00, -8.3296e-01,  1.7708e-01,\n",
       "          2.6387e-01,  2.2537e+00,  7.2226e-01, -2.0205e+00, -1.8105e-01,\n",
       "         -1.6518e+00, -4.8895e-01, -4.4622e-02, -1.0224e+00,  7.1648e-01,\n",
       "          5.1220e-01,  1.4968e-01,  1.6754e-01, -3.6131e-01,  4.9489e-01,\n",
       "          1.1083e-01,  1.8192e+00, -6.1170e-01, -5.8401e-01, -3.8023e-01,\n",
       "         -9.6099e-01,  1.1824e+00,  1.6529e+00,  3.3776e-01,  1.6838e-01,\n",
       "         -7.7459e-01, -1.8677e+00,  8.2216e-01, -5.0338e-01, -1.4278e+00,\n",
       "          3.6508e-02,  8.6979e-01, -1.0263e+00,  8.7361e-01, -3.0444e-01,\n",
       "         -6.4135e-01, -1.0152e+00, -8.0522e-01,  7.4396e-01, -4.8787e-01,\n",
       "         -1.0840e+00, -1.4669e-01, -2.4248e+00, -1.2483e+00,  8.7791e-01,\n",
       "          7.1821e-01, -8.4397e-02, -1.9285e-01, -7.7179e-01, -1.3631e-01,\n",
       "          1.4930e-01,  5.6266e-01,  2.3210e+00, -1.4621e-01,  2.0799e+00,\n",
       "         -2.1830e+00, -2.0014e+00,  6.3792e-02, -2.1899e+00,  1.7420e+00,\n",
       "          1.7357e+00, -1.6215e+00,  7.6487e-01, -1.0913e+00,  2.0870e+00,\n",
       "         -1.1308e+00,  2.2846e+00, -1.9631e+00,  4.2961e-01,  1.3115e+00,\n",
       "         -2.0319e+00,  2.0272e+00,  1.9380e+00,  1.5353e+00,  5.2837e-01,\n",
       "         -2.4688e+00,  9.0775e-01, -6.8548e-01,  1.3569e+00, -1.1043e+00,\n",
       "          5.0968e-01, -1.2987e+00, -1.9582e+00,  4.4944e-01,  9.0380e-01,\n",
       "         -1.2120e+00,  8.3882e-01,  2.1062e-01, -1.4422e+00, -1.8854e+00,\n",
       "          1.0704e-01,  1.5841e-01,  8.9530e-01,  1.6188e+00,  3.9080e-01,\n",
       "          1.5534e+00,  1.7194e-02,  7.7649e-02, -5.4027e-01,  2.2006e+00,\n",
       "          5.5913e-01, -4.7499e-01, -7.4610e-01,  1.5577e-01, -1.3100e+00,\n",
       "          1.2606e-01,  1.0063e+00,  1.3110e-01,  6.5860e-03, -1.0478e+00,\n",
       "          1.4436e+00, -1.0520e-01,  1.0002e+00, -7.6131e-01, -1.5882e-01,\n",
       "          1.0786e+00,  1.5289e+00, -5.4058e-01, -4.3839e-01,  1.0389e+00,\n",
       "          1.6872e+00, -1.3966e-01, -9.4544e-01,  8.1351e-01, -5.8842e-01,\n",
       "         -2.3412e+00, -4.8615e-01,  1.3194e+00, -9.9860e-01, -5.7291e-01,\n",
       "         -2.2596e-01, -2.7246e-01,  1.5503e-01, -1.0779e-01, -8.4040e-01,\n",
       "          6.0735e-01, -7.5399e-01,  7.1137e-01,  1.3678e+00, -6.5077e-01,\n",
       "          7.8987e-01,  5.3207e-01,  2.0779e+00, -1.2808e+00,  1.4959e-01,\n",
       "         -1.0426e+00,  7.3539e-02,  4.8789e-01, -2.5747e-01, -1.1764e+00,\n",
       "          6.7346e-01,  1.9356e+00, -6.1113e-01, -1.1336e-01,  1.0085e+00,\n",
       "          7.2338e-01, -1.0901e+00, -1.4771e+00,  3.3031e-01,  4.2854e-01,\n",
       "          7.4867e-02, -1.2403e+00,  1.3213e+00, -1.2566e+00,  2.6058e+00,\n",
       "          1.4981e+00, -1.0783e+00, -7.4998e-01, -1.4996e+00, -1.2733e-01,\n",
       "          2.5809e+00, -3.1853e-01, -1.2087e+00, -3.9369e-01, -2.0794e+00,\n",
       "         -2.8653e-01,  1.3900e+00, -8.4798e-01, -1.0996e+00, -6.9493e-02,\n",
       "          1.0601e+00, -3.4157e-01, -3.1979e-02,  1.8366e+00,  1.7769e+00,\n",
       "         -1.3746e+00, -2.2785e+00,  4.3730e-01, -1.1843e+00,  6.3105e-01,\n",
       "          7.9212e-02, -7.7005e-02,  1.7693e-01, -2.4401e-01, -1.9486e-01,\n",
       "         -1.1210e+00, -1.2951e+00, -1.1222e+00, -4.3237e-01,  9.5688e-02,\n",
       "         -5.6278e-01, -7.8993e-01, -1.5090e+00,  6.7651e-02,  1.1656e-01,\n",
       "         -1.9892e-01, -7.8195e-01, -1.8176e+00, -9.9806e-01,  1.0273e+00,\n",
       "         -6.0225e-01, -1.7492e-01, -1.0668e-01,  1.5166e+00, -6.0958e-02,\n",
       "          1.9720e+00,  6.9189e-01, -5.8124e-01,  1.3110e+00, -7.7729e-01,\n",
       "          1.3143e+00, -1.6945e+00, -1.4122e+00,  1.6694e+00,  1.5687e-01,\n",
       "          6.1231e-01,  1.5098e+00,  2.3515e-01,  8.4711e-01, -8.5484e-01,\n",
       "         -2.1883e-02,  2.4416e-01,  8.4414e-02,  1.2192e+00,  7.3837e-01,\n",
       "          3.9938e-01, -4.4370e-01, -2.1298e-01, -4.3985e-01, -8.5348e-01,\n",
       "         -2.3802e+00, -6.2497e-01, -1.7744e+00,  3.4167e+00, -1.0681e+00,\n",
       "          9.9724e-01,  8.0698e-01, -7.6353e-01,  3.2037e+00,  9.3326e-01,\n",
       "          1.4617e+00,  2.5410e-01,  8.6793e-01,  2.2467e-01, -3.3651e-01,\n",
       "          4.6527e-01,  2.4224e-02,  4.5491e-01,  2.6953e+00,  4.0297e-01,\n",
       "         -3.8323e-01,  6.8360e-01, -5.9403e-01, -8.1102e-03, -9.0693e-01,\n",
       "         -1.1315e+00,  2.1727e+00,  1.0051e+00,  9.6691e-04,  1.3383e+00,\n",
       "          3.0319e-01, -7.4506e-01, -2.2307e-01, -2.2917e+00, -2.2052e-01,\n",
       "          8.4224e-01,  1.6849e+00,  7.3725e-02,  8.4215e-01,  3.8109e-01,\n",
       "         -2.7181e-02, -1.1448e+00,  3.4734e-01, -1.9707e+00,  1.2277e+00,\n",
       "          3.7357e-01, -1.3731e+00,  2.0660e-01,  5.9901e-01, -1.3342e+00,\n",
       "          1.5641e+00,  1.5403e-01,  2.9341e+00, -3.3332e-01, -1.7456e+00,\n",
       "         -3.0481e-01,  6.0587e-01, -7.8385e-01, -1.3971e+00,  4.4360e-01,\n",
       "         -3.2750e-01, -3.7334e-01,  3.4727e-01,  2.4840e+00,  2.3003e-01,\n",
       "          5.1393e-01,  2.4753e-01, -8.3333e-02, -8.9028e-02,  9.7017e-01,\n",
       "          1.2671e+00, -1.4924e+00, -9.7307e-01, -6.0868e-01,  4.9109e-01,\n",
       "         -1.5817e-01, -3.2062e-02, -6.7728e-01,  2.1887e+00,  1.4837e+00,\n",
       "         -1.5904e+00, -1.1386e-01, -5.1608e-01, -5.4303e-01,  9.4799e-02,\n",
       "          2.4637e+00, -1.5095e+00, -1.4937e+00,  1.0474e+00,  1.5587e+00,\n",
       "          1.2215e+00, -1.1891e+00,  2.3852e-01,  1.2330e+00, -2.3797e-01,\n",
       "         -1.4318e+00,  7.2875e-01,  1.8617e+00, -2.4325e+00, -9.9885e-01,\n",
       "          9.4900e-01,  5.3327e-01,  1.4214e+00,  1.9354e+00, -4.0799e-01,\n",
       "          8.4387e-01,  1.8840e+00, -2.2393e-01, -1.0529e+00, -7.0980e-01,\n",
       "         -4.5764e-01, -6.3300e-01,  9.6423e-02,  1.9207e+00, -8.7396e-01,\n",
       "          4.7285e-01,  7.3092e-01, -1.2872e-01,  3.0367e+00,  9.6229e-01,\n",
       "          1.1943e+00,  4.3139e-01,  9.0145e-01, -2.3708e+00,  1.3727e+00,\n",
       "          4.2132e-01, -2.1038e-02, -2.2271e-01,  3.4174e-01,  1.0763e+00,\n",
       "         -6.4774e-01, -5.2793e-01, -5.2977e-01]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vectors[:,-3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9b933454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b30fe9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method decode in module transformers.tokenization_utils_base:\n",
      "\n",
      "decode(token_ids: Union[int, List[int], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = True, **kwargs) -> str method of transformers.models.flaubert.tokenization_flaubert.FlaubertTokenizer instance\n",
      "    Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
      "    tokens and clean up tokenization spaces.\n",
      "    \n",
      "    Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.\n",
      "    \n",
      "    Args:\n",
      "        token_ids (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      "            List of tokenized input ids. Can be obtained using the `__call__` method.\n",
      "        skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to remove special tokens in the decoding.\n",
      "        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not to clean up the tokenization spaces.\n",
      "        kwargs (additional keyword arguments, *optional*):\n",
      "            Will be passed to the underlying model specific decode method.\n",
      "    \n",
      "    Returns:\n",
      "        `str`: The decoded sentence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(flaubert_tokenizer.decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "00f145b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<special1>'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flaubert_tokenizer.decode(tokens[0,2],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f494a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62170c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf6516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3c05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e4fd47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use joblib to fast your function\n",
    "\n",
    "\n",
    "def decode(tokenizer, pred_idx, top_clean):\n",
    "  ignore_tokens =  '[PAD]'\n",
    "  tokens = []\n",
    "  for w in pred_idx:\n",
    "    token = ''.join(tokenizer.decode(w).split())\n",
    "    if token not in ignore_tokens:\n",
    "      tokens.append(token.replace('##', ''))\n",
    "  return '\\n'.join(tokens[:top_clean])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def encode(tokenizer, text_sentence, add_special_tokens=True):\n",
    "  text_sentence = text_sentence.replace('<mask>', tokenizer.mask_token)\n",
    "    # if <mask> is the last token, append a \".\" so that models dont predict punctuation.\n",
    "  if tokenizer.mask_token == text_sentence.split()[-1]:\n",
    "    text_sentence += ' .'\n",
    "\n",
    "    input_ids = torch.tensor([tokenizer.encode(text_sentence, add_special_tokens=add_special_tokens)])\n",
    "    mask_idx = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "  return input_ids, mask_idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_all_predictions(text_sentence, top_clean=5):\n",
    "    # ========================= BERT =================================\n",
    "  input_ids, mask_idx = encode(flaubert_tokenizer, text_sentence)\n",
    "  with torch.no_grad():\n",
    "    predict = flaubert(input_ids)[0]\n",
    "  print(predict)\n",
    "  bert = decode(flaubert_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n",
    "  return {'bert': bert}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_prediction_eos(input_text):\n",
    "    input_text += ' <special1>'\n",
    "    res = get_all_predictions(input_text, top_clean=int(top_k))\n",
    "    print(input_text)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2260c62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 6.5603e-01, -1.8916e-01,  6.6605e-04,  ...,  1.0323e+00,\n",
      "          -5.9226e-01, -4.0247e-01],\n",
      "         [ 4.3116e-01,  2.0087e-01,  1.7973e-02,  ...,  1.1723e+00,\n",
      "          -7.4666e-01, -4.2661e-01],\n",
      "         [ 6.9783e-03,  1.0165e+00, -3.2521e-01,  ...,  1.7693e+00,\n",
      "          -3.3322e-01, -6.7596e-01],\n",
      "         ...,\n",
      "         [ 4.8239e-01,  2.6933e-01,  1.7991e-01,  ...,  1.4804e+00,\n",
      "          -5.1925e-01, -2.5931e-01],\n",
      "         [ 3.1507e-01,  9.0788e-01, -2.0803e-01,  ...,  1.6522e+00,\n",
      "          -2.8450e-01, -7.5895e-01],\n",
      "         [ 7.6024e-02,  6.2625e-01, -5.0215e-01,  ...,  1.6336e+00,\n",
      "          -1.2954e-01, -6.7332e-01]]])\n",
      "Je vais boire de la  <special1>\n",
      "{'bert': '<special7>'}\n"
     ]
    }
   ],
   "source": [
    "top_k=1\n",
    "a=get_prediction_eos(\"Je vais boire de la \")\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
